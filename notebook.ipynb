{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e2df117",
   "metadata": {},
   "source": [
    "# pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed54e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lightgbm scikit-learn xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5315f7cd",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "6c93681c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:51:39.513170Z",
     "start_time": "2025-11-26T09:51:37.921221Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data preparation\n",
    "We will check for missing data, numerically encode categorical variables, and do some feature engineering in regards to potential oil price lags."
   ],
   "id": "d662c64b"
  },
  {
   "cell_type": "code",
   "id": "4f03f96c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:51:45.090367Z",
     "start_time": "2025-11-26T09:51:43.997419Z"
    }
   },
   "source": [
    "# Notebook location\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "# Data directory\n",
    "DATA_DIR = NOTEBOOK_DIR / \"data\"\n",
    "\n",
    "#parse_dates=['date'] converts date values into datetime objects\n",
    "train = pd.read_csv(DATA_DIR / \"train.csv\", parse_dates=['date'])\n",
    "test = pd.read_csv(DATA_DIR / 'test.csv', parse_dates=['date'])\n",
    "holidays_events = pd.read_csv(DATA_DIR / 'holidays_events.csv', parse_dates=['date'])\n",
    "oil = pd.read_csv(DATA_DIR / 'oil.csv', parse_dates=['date'])\n",
    "stores = pd.read_csv(DATA_DIR / 'stores.csv')\n",
    "transactions = pd.read_csv(DATA_DIR / 'transactions.csv', parse_dates=['date'])\n",
    "#data = pd.read_csv(DATA_DIR / 'data.csv', parse_dates=['date'])"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "31142406",
   "metadata": {},
   "source": [
    "### Check if there are missing dates in the train data"
   ]
  },
  {
   "cell_type": "code",
   "id": "ef8903c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:51:47.366878Z",
     "start_time": "2025-11-26T09:51:47.340539Z"
    }
   },
   "source": [
    "# Get the date range from earliest to latest in train data\n",
    "date_range = pd.date_range(start=train['date'].min(), end=train['date'].max(), freq='D')\n",
    "\n",
    "# Get unique dates in train data\n",
    "train_dates = train['date'].unique()\n",
    "\n",
    "# Find missing dates\n",
    "missing_dates = date_range.difference(pd.DatetimeIndex(train_dates))\n",
    "\n",
    "print(f\"Total dates in range: {len(date_range)}\")\n",
    "print(f\"Unique dates in train: {len(train_dates)}\")\n",
    "print(f\"Missing dates: {len(missing_dates)}\")\n",
    "print(f\"\\nMissing dates:\\n{missing_dates}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dates in range: 1688\n",
      "Unique dates in train: 1684\n",
      "Missing dates: 4\n",
      "\n",
      "Missing dates:\n",
      "DatetimeIndex(['2013-12-25', '2014-12-25', '2015-12-25', '2016-12-25'], dtype='datetime64[ns]', freq=None)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have 4 missing dates in total. This will cause problems in modelling, since timeseries models require a continuous flow time. Forecasting models require need the timeseries input to be complete, not missing any dates.  The following code will fix this and set the target value of sales as 0 on these days.",
   "id": "8f3b2f7ed717a3e6"
  },
  {
   "cell_type": "code",
   "id": "3d499885",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:51:54.640245Z",
     "start_time": "2025-11-26T09:51:52.249806Z"
    }
   },
   "source": [
    "# Create a complete date range from train data\n",
    "date_range = pd.date_range(start=train['date'].min(), end=train['date'].max(), freq='D')\n",
    "\n",
    "# Get all unique combinations of store number and product family\n",
    "stores_list = train['store_nbr'].unique()\n",
    "unique_product_families = train['family'].unique()\n",
    "\n",
    "# Create a MultiIndex with all combinations of date, store_nbr, and family\n",
    "complete_index = pd.MultiIndex.from_tuples(\n",
    "    product(date_range, stores_list, unique_product_families),\n",
    "    names=['date', 'store_nbr', 'family']\n",
    ")\n",
    "\n",
    "# Create a complete dataframe\n",
    "complete_df = pd.DataFrame(index=complete_index).reset_index()\n",
    "\n",
    "# Merge with original train data\n",
    "train_complete = complete_df.merge(\n",
    "    train,\n",
    "    on=['date', 'store_nbr', 'family'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing values i.e. sales and onpromotion with 0\n",
    "train_complete['sales'] = train_complete['sales'].fillna(0)\n",
    "train_complete['onpromotion'] = train_complete['onpromotion'].fillna(0)\n",
    "\n",
    "# 4 missing dates, 54 stores, 33 product families i.e. 4 * 54 * 33 = 7128\n",
    "print(f\"Added records: {len(train_complete) - len(train)}\")\n",
    "\n",
    "train = train_complete"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added records: 7128\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "f6b9e5f4",
   "metadata": {},
   "source": [
    "### Check missing oil dates"
   ]
  },
  {
   "cell_type": "code",
   "id": "71bae50b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:51:57.306008Z",
     "start_time": "2025-11-26T09:51:57.293865Z"
    }
   },
   "source": [
    "# Get the date range from earliest to latest in train and test data\n",
    "date_range = pd.date_range(start=train['date'].min(), end=test['date'].max(), freq='D')\n",
    "\n",
    "# Get unique dates in oil data\n",
    "oil_dates = oil['date'].unique()\n",
    "\n",
    "# Find missing dates\n",
    "missing_dates = date_range.difference(pd.DatetimeIndex(oil_dates))\n",
    "\n",
    "print(f\"Total dates in range: {len(date_range)}\")\n",
    "print(f\"Unique dates in oil: {len(oil_dates)}\")\n",
    "print(f\"Missing dates: {len(missing_dates)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dates in range: 1704\n",
      "Unique dates in oil: 1218\n",
      "Missing dates: 486\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We are once again missing dates, which disrupts our continuous timeseries. We will use the same method to fix it.",
   "id": "375441b171bef850"
  },
  {
   "cell_type": "code",
   "id": "32564df9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:53:19.483738Z",
     "start_time": "2025-11-26T09:53:19.452498Z"
    }
   },
   "source": [
    "# Create complete date range for the data from train and test data\n",
    "date_range = pd.date_range(start=train['date'].min(), end=test['date'].max(), freq='D')\n",
    "\n",
    "# Create complete dataframe with all dates\n",
    "oil_complete = pd.DataFrame({'date': date_range})\n",
    "\n",
    "# Merge with original oil data\n",
    "oil_complete = oil_complete.merge(\n",
    "    oil, \n",
    "    on='date', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Forward fill (use last known price for missing dates)\n",
    "oil_complete['dcoilwtico'] = oil_complete['dcoilwtico'].ffill()\n",
    "\n",
    "# Backward fill for any remaining NaNs at the start\n",
    "oil_complete['dcoilwtico'] = oil_complete['dcoilwtico'].bfill()\n",
    "\n",
    "# Update oil dataframe\n",
    "print(f\"Number of records after filling missing oil data: {len(oil_complete)}\")\n",
    "print(f\"Added records: {len(oil_complete) - len(oil)}\")\n",
    "\n",
    "oil = oil_complete"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records after filling missing oil data: 1704\n",
      "Added records: 486\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "1dfcb5c5",
   "metadata": {},
   "source": [
    "### Check transactions"
   ]
  },
  {
   "cell_type": "code",
   "id": "52a01d57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:54:05.247200Z",
     "start_time": "2025-11-26T09:54:04.939127Z"
    }
   },
   "source": [
    "# Total transactions count\n",
    "total_transactions_count = len(train.groupby([\"date\", \"store_nbr\"])['sales'].sum())\n",
    "\n",
    "print(f\"Missing records from transactions: {total_transactions_count - len(transactions)}\")\n",
    "\n",
    "\n",
    "store_sales = train.groupby(['date', 'store_nbr'])['sales'].sum().reset_index()\n",
    "\n",
    "# Merge transactions with sales\n",
    "transactions_complete = transactions.merge(\n",
    "    store_sales,\n",
    "    on=['date', 'store_nbr'],\n",
    "    how='outer'\n",
    ").sort_values(by=['date', 'store_nbr'], ignore_index=True)\n",
    "\n",
    "# For dates where sales are 0, set transactions to 0\n",
    "transactions_complete.loc[transactions_complete[\"sales\"].eq(0), \"transactions\"] = 0\n",
    "\n",
    "# Interpolate missing transactions for each store\n",
    "transactions_complete['transactions'] = transactions_complete.groupby('store_nbr')['transactions'].transform(\n",
    "    lambda x: x.interpolate(method='linear')\n",
    ")\n",
    "\n",
    "# Round transactions to remove fractions from interpolation\n",
    "transactions_complete['transactions'] = transactions_complete['transactions'].round().astype(int)\n",
    "\n",
    "# Drop sales column\n",
    "transactions_complete = transactions_complete.drop('sales', axis=1)\n",
    "\n",
    "print(f\"Added transactions: {len(transactions_complete) - len(transactions)}\")\n",
    "\n",
    "transactions = transactions_complete"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing records from transactions: 7664\n",
      "Added transactions: 7664\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "7fa25219",
   "metadata": {},
   "source": [
    "### Holidays"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We want to create a 0/1 flag variable for holidays in our time series. This way we can account for extra store traffic during holidays quite simply.",
   "id": "1643d742c0c35da1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T17:01:17.438129Z",
     "start_time": "2025-11-27T17:01:15.170541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Clean holiday event descriptions and remove \"Transfer\" holidays, since they are actually not holidays.\n",
    "holidays_events[\"description\"] = holidays_events.apply(\n",
    "    lambda x: x[\"description\"].strip().lower().replace(x[\"locale_name\"].lower(), \"\"), axis=1\n",
    ").apply(\n",
    "    #If the holiday is connected to football, we will just categorize it as futbol\n",
    "    lambda x: \"futbol\" if \"futbol\" in x else x\n",
    ").replace(\n",
    "    #Removing unneeded words in the names of holidays\n",
    "    r'\\b(de|del|puente|traslado|recupero)\\b', '', regex=True\n",
    ").replace(\n",
    "    #Removing +1, -2 etc type holidays since these are not actual holidays\n",
    "    r'[+-]\\d+', '', regex=True\n",
    ").str.strip()\n",
    "#Transfers are not actually holidays, they have been moved to a different day.\n",
    "holidays_events = holidays_events[holidays_events[\"type\"] != \"Transfer\"]\n",
    "\n",
    "#Build 3 holiday tables for national, regional and local\n",
    "\n",
    "h = holidays_events[['date', 'locale', 'locale_name']].copy()\n",
    "\n",
    "# NATIONAL\n",
    "#We only need to keep the date, since nationals take place all over the country\n",
    "national = h[h['locale'] == 'National'][['date']].drop_duplicates()\n",
    "national['national_holiday'] = 1\n",
    "\n",
    "# REGIONAL → matches on state\n",
    "#we have to keep the date and the region\n",
    "regional = h[h['locale'] == 'Regional'][['date', 'locale_name']].drop_duplicates()\n",
    "regional = regional.rename(columns={'locale_name': 'state'})\n",
    "regional['regional_holiday'] = 1\n",
    "\n",
    "# LOCAL → matches on city\n",
    "#We have to keep the city and date\n",
    "local = h[h['locale'] == 'Local'][['date', 'locale_name']].drop_duplicates()\n",
    "local = local.rename(columns={'locale_name': 'city'})\n",
    "local['local_holiday'] = 1\n",
    "\n",
    "# --- Merge into data ---\n",
    "data = pd.concat(\n",
    "    [train, test], axis=0, ignore_index=True\n",
    ").merge(\n",
    "    transactions, on=['date', 'store_nbr'], how='left'\n",
    ").merge(\n",
    "    oil, on='date', how='left'\n",
    ").merge(\n",
    "    stores, on='store_nbr', how='left'\n",
    ")\n",
    "# Merge NATIONAL\n",
    "data = data.merge(national, on='date', how='left')\n",
    "\n",
    "# Merge REGIONAL\n",
    "data = data.merge(regional, on=['date', 'state'], how='left')\n",
    "\n",
    "# Merge LOCAL\n",
    "data = data.merge(local, on=['date', 'city'], how='left')\n",
    "\n",
    "# Final combined holiday flag column\n",
    "data['holiday'] = (\n",
    "    data[['national_holiday', 'regional_holiday', 'local_holiday']]\n",
    "    .fillna(0)\n",
    "    .max(axis=1)\n",
    "    .astype(int) #should maybe make into factor\n",
    ")\n",
    "\n",
    "# Cleanup\n",
    "data = data.drop(columns=['national_holiday', 'regional_holiday', 'local_holiday'])\n",
    "\n",
    "data['holiday'].value_counts()"
   ],
   "id": "fa8121b4bd10a904",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "holiday\n",
       "0    2774310\n",
       "1     262218\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "2e5c9a14",
   "metadata": {},
   "source": [
    "### Creating one and two day lags for dcoilwtico"
   ]
  },
  {
   "cell_type": "code",
   "id": "f42a9765",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T17:01:32.759357Z",
     "start_time": "2025-11-27T17:01:31.328167Z"
    }
   },
   "source": [
    "# Sort by store_nbr, family, and date to ensure correct order within each group\n",
    "data = data.sort_values(['store_nbr', 'family', 'date'])\n",
    "\n",
    "# Create lag-1 column within each store-family combination\n",
    "data['lag-1-dcoilwtico'] = data.groupby(['store_nbr', 'family'])['dcoilwtico'].shift(1)\n",
    "\n",
    "# Create lag-2 column (2 days ago)\n",
    "data['lag-2-dcoilwtico'] = data.groupby(['store_nbr', 'family'])['dcoilwtico'].shift(2)\n",
    "\n",
    "# Backfill NaNs at the start of each group\n",
    "data['lag-1-dcoilwtico'] = data.groupby(['store_nbr', 'family'])['lag-1-dcoilwtico'].bfill()\n",
    "data['lag-2-dcoilwtico'] = data.groupby(['store_nbr', 'family'])['lag-2-dcoilwtico'].bfill()\n",
    "data = data.sort_values('date')"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "eab82a7a",
   "metadata": {},
   "source": [
    "### Extract date features"
   ]
  },
  {
   "cell_type": "code",
   "id": "32ab3675",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T17:01:36.004760Z",
     "start_time": "2025-11-27T17:01:35.558332Z"
    }
   },
   "source": [
    "data['year'] = data['date'].dt.year\n",
    "data['month'] = data['date'].dt.month\n",
    "data['day'] = data['date'].dt.day\n",
    "data['day_of_week'] = data['date'].dt.dayofweek\n",
    "data['day_of_year'] = data['date'].dt.dayofyear\n",
    "data['week_of_year'] = data['date'].dt.isocalendar().week\n",
    "data['is_weekend'] = (data['date'].dt.dayofweek >= 5).astype(int)"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "96f579f3",
   "metadata": {},
   "source": [
    "### Label Encoding for categorical features"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Since most ML models cannot work with categorical features like \"AUTOMOTIVE\" etc, we have to numerically encode them for the model to work.",
   "id": "81fbb111f774a079"
  },
  {
   "cell_type": "code",
   "id": "7e9bbb6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T17:01:43.943246Z",
     "start_time": "2025-11-27T17:01:42.758983Z"
    }
   },
   "source": [
    "# Get all unique values for each categorical column from data\n",
    "all_product_families = data['family'].unique()\n",
    "all_cities = data['city'].unique()\n",
    "all_states = data['state'].unique()\n",
    "all_types = data['type'].unique()\n",
    "\n",
    "# Initialize label encoders\n",
    "le_product_family = LabelEncoder()\n",
    "le_city = LabelEncoder()\n",
    "le_state = LabelEncoder()\n",
    "le_type = LabelEncoder()\n",
    "\n",
    "# Fit on all unique values\n",
    "le_product_family.fit(all_product_families)\n",
    "le_city.fit(all_cities)\n",
    "le_state.fit(all_states)\n",
    "le_type.fit(all_types)\n",
    "\n",
    "# Transform data dataframe\n",
    "data['family'] = le_product_family.transform(data['family'])\n",
    "data['city'] = le_city.transform(data['city'])\n",
    "data['state'] = le_state.transform(data['state'])\n",
    "data['type'] = le_type.transform(data['type'])\n",
    "\n",
    "print(\"Label encoding complete!\")\n",
    "print(f\"Family: {len(all_product_families)} categories\")\n",
    "print(f\"City: {len(all_cities)} categories\")\n",
    "print(f\"State: {len(all_states)} categories\")\n",
    "print(f\"Type: {len(all_types)} categories\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoding complete!\n",
      "Family: 33 categories\n",
      "City: 22 categories\n",
      "State: 16 categories\n",
      "Type: 5 categories\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "56ae8e71",
   "metadata": {},
   "source": [
    "### Save final version of 'data' dataframe "
   ]
  },
  {
   "cell_type": "code",
   "id": "8e90d795",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T17:02:06.320477Z",
     "start_time": "2025-11-27T17:01:50.158026Z"
    }
   },
   "source": [
    "data.to_csv('./data/data.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "5fb22739",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67725cad",
   "metadata": {},
   "source": [
    "### Splitting test and train data"
   ]
  },
  {
   "cell_type": "code",
   "id": "e6ca108d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T17:03:40.312544Z",
     "start_time": "2025-11-27T17:03:38.951916Z"
    }
   },
   "source": [
    "# Split based on date\n",
    "train = data[data['date'] < '2017-08-16'].copy()\n",
    "test = data[data['date'] >= '2017-08-16'].copy()\n",
    "\n",
    "# Save test IDs before dropping\n",
    "test_ids = test['id'].copy()\n",
    "\n",
    "# Drop transactions column from both\n",
    "train = train.drop('transactions', axis=1)\n",
    "test = test.drop('transactions', axis=1)\n",
    "\n",
    "# Drop date column since we have date features\n",
    "train = train.drop('date', axis=1)\n",
    "test = test.drop('date', axis=1)\n",
    "\n",
    "# Drop id column\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)\n",
    "\n",
    "# Drop sales column only from test\n",
    "test = test.drop('sales', axis=1)\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"Test IDs saved: {len(test_ids)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (3008016, 19)\n",
      "Test shape: (28512, 18)\n",
      "Test IDs saved: 28512\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "d12a9368",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T17:03:56.664472Z",
     "start_time": "2025-11-27T17:03:56.424432Z"
    }
   },
   "source": [
    "# Separate features and target\n",
    "X_train = train.drop('sales', axis=1)\n",
    "y_train = train['sales']\n",
    "\n",
    "X_test = test \n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Target: {y_train.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (3008016, 18)\n",
      "Test set: (28512, 18)\n",
      "Target: (3008016,)\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "5fc8877c",
   "metadata": {},
   "source": [
    "### Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "id": "2ced950f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T17:04:08.183519Z",
     "start_time": "2025-11-27T17:04:08.178524Z"
    }
   },
   "source": [
    "def train_random_forest_model(X_train, y_train):\n",
    "    # Initialize Random Forest model\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=20,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=4,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training Random Forest model...\")\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on training set to evaluate\n",
    "    y_train_pred_rf = np.maximum(0, rf_model.predict(X_train))\n",
    "\n",
    "    # Calculate metrics\n",
    "    rmsle_score = np.sqrt(mean_squared_log_error(y_train, y_train_pred_rf))\n",
    "\n",
    "    return rf_model, rmsle_score"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "781d3f5f",
   "metadata": {},
   "source": [
    "### LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "b9bdd511",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T17:04:14.708127Z",
     "start_time": "2025-11-27T17:04:14.704839Z"
    }
   },
   "source": [
    "def train_lightgbm_model(X_train, y_train):\n",
    "    # Initialize LightGBM model\n",
    "    lgb_model = LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=20,\n",
    "        learning_rate=0.1,\n",
    "        num_leaves=31,\n",
    "        min_child_samples=20,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training LightGBM model...\")\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on training set to evaluate\n",
    "    y_train_pred_lgb = np.maximum(0, lgb_model.predict(X_train))\n",
    "\n",
    "    # Calculate metrics\n",
    "    rmsle_score_lgb = np.sqrt(mean_squared_log_error(y_train, y_train_pred_lgb))\n",
    "\n",
    "    return lgb_model, rmsle_score_lgb"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "id": "05df1d02",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "a4270622",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T17:04:17.881126Z",
     "start_time": "2025-11-27T17:04:17.877751Z"
    }
   },
   "source": [
    "def train_xgboost_model(X_train, y_train):\n",
    "    # Initialize XGBoost model\n",
    "    xgb_model = XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        learning_rate=0.1,\n",
    "        min_child_weight=3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbosity=1\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training XGBoost model...\")\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on training set to evaluate\n",
    "    y_train_pred_xgb = np.maximum(0, xgb_model.predict(X_train))\n",
    "\n",
    "    # Calculate metrics\n",
    "    rmsle_score_xgb = np.sqrt(mean_squared_log_error(y_train, y_train_pred_xgb))\n",
    "\n",
    "    return xgb_model, rmsle_score_xgb"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "2d3c7bf7",
   "metadata": {},
   "source": [
    "### Create submission"
   ]
  },
  {
   "cell_type": "code",
   "id": "5dbfa36e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T17:04:25.189265Z",
     "start_time": "2025-11-27T17:04:25.186172Z"
    }
   },
   "source": [
    "def create_submission(predictions, test_ids, filename='submission.csv'):\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_ids.values.astype(int),\n",
    "        'sales': predictions\n",
    "    })\n",
    "    \n",
    "    submission.to_csv(filename, index=False)\n",
    "    return submission"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "id": "7fbbd718",
   "metadata": {},
   "source": [
    "### Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "id": "9d5b159c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T17:12:21.229702Z",
     "start_time": "2025-11-27T17:04:30.462484Z"
    }
   },
   "source": [
    "xgb_model, xgb_rmsle_score = train_xgboost_model(X_train, y_train)\n",
    "rf_model, rf_rmsle_score = train_random_forest_model(X_train, y_train)\n",
    "lgbm_model, lgbm_rmsle_score = train_lightgbm_model(X_train, y_train)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost model...\n",
      "Training Random Forest model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  7.3min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:   11.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM model...\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "e54eba17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T17:15:26.486387Z",
     "start_time": "2025-11-27T17:15:26.455163Z"
    }
   },
   "source": [
    "# Compare all models including ensemble\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'LightGBM', 'XGBoost'],\n",
    "    'RMSLE': [rf_rmsle_score, lgbm_rmsle_score, xgb_rmsle_score]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(f\"\\nBest model by RMSLE: {comparison_df.loc[comparison_df['RMSLE'].idxmin(), 'Model']}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance Comparison:\n",
      "        Model    RMSLE\n",
      "Random Forest 0.625781\n",
      "     LightGBM 2.154993\n",
      "      XGBoost 1.505900\n",
      "\n",
      "Best model by RMSLE: Random Forest\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "id": "f1e3cea4",
   "metadata": {},
   "source": [
    "### Create submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7987718b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All submission files created successfully!\n"
     ]
    }
   ],
   "source": [
    "xgb_y_pred = xgb_model.predict(X_test)\n",
    "rf_y_pred = rf_model.predict(X_test)\n",
    "lgbm_y_pred = lgbm_model.predict(X_test)\n",
    "\n",
    "# Create submission files using the saved test IDs\n",
    "xgb_submission = create_submission(xgb_y_pred, test_ids, 'xgb_submission.csv')\n",
    "rf_submission = create_submission(rf_y_pred, test_ids, 'rf_submission.csv')\n",
    "lgbm_submission = create_submission(lgbm_y_pred, test_ids, 'lgbm_submission.csv')\n",
    "\n",
    "print(\"All submission files created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
