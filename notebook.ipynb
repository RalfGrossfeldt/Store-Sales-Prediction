{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e2df117",
   "metadata": {},
   "source": [
    "# pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed54e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lightgbm scikit-learn xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5315f7cd",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "6c93681c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:51:39.513170Z",
     "start_time": "2025-11-26T09:51:37.921221Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data preparation\n",
    "We will check for missing data, numerically encode categorical variables, and do some feature engineering in regards to potential oil price lags."
   ],
   "id": "d662c64b"
  },
  {
   "cell_type": "code",
   "id": "4f03f96c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:51:45.090367Z",
     "start_time": "2025-11-26T09:51:43.997419Z"
    }
   },
   "source": [
    "# Notebook location\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "# Data directory\n",
    "DATA_DIR = NOTEBOOK_DIR / \"data\"\n",
    "\n",
    "#parse_dates=['date'] converts date values into datetime objects\n",
    "train = pd.read_csv(DATA_DIR / \"train.csv\", parse_dates=['date'])\n",
    "test = pd.read_csv(DATA_DIR / 'test.csv', parse_dates=['date'])\n",
    "holidays_events = pd.read_csv(DATA_DIR / 'holidays_events.csv', parse_dates=['date'])\n",
    "oil = pd.read_csv(DATA_DIR / 'oil.csv', parse_dates=['date'])\n",
    "stores = pd.read_csv(DATA_DIR / 'stores.csv')\n",
    "transactions = pd.read_csv(DATA_DIR / 'transactions.csv', parse_dates=['date'])\n",
    "#data = pd.read_csv(DATA_DIR / 'data.csv', parse_dates=['date'])"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "31142406",
   "metadata": {},
   "source": [
    "### Check if there are missing dates in the train data"
   ]
  },
  {
   "cell_type": "code",
   "id": "ef8903c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:51:47.366878Z",
     "start_time": "2025-11-26T09:51:47.340539Z"
    }
   },
   "source": [
    "# Get the date range from earliest to latest in train data\n",
    "date_range = pd.date_range(start=train['date'].min(), end=train['date'].max(), freq='D')\n",
    "\n",
    "# Get unique dates in train data\n",
    "train_dates = train['date'].unique()\n",
    "\n",
    "# Find missing dates\n",
    "missing_dates = date_range.difference(pd.DatetimeIndex(train_dates))\n",
    "\n",
    "print(f\"Total dates in range: {len(date_range)}\")\n",
    "print(f\"Unique dates in train: {len(train_dates)}\")\n",
    "print(f\"Missing dates: {len(missing_dates)}\")\n",
    "print(f\"\\nMissing dates:\\n{missing_dates}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dates in range: 1688\n",
      "Unique dates in train: 1684\n",
      "Missing dates: 4\n",
      "\n",
      "Missing dates:\n",
      "DatetimeIndex(['2013-12-25', '2014-12-25', '2015-12-25', '2016-12-25'], dtype='datetime64[ns]', freq=None)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have 4 missing dates in total. This will cause problems in modelling, since timeseries models require a continuous flow time. Forecasting models require need the timeseries input to be complete, not missing any dates.  The following code will fix this and set the target value of sales as 0 on these days.",
   "id": "8f3b2f7ed717a3e6"
  },
  {
   "cell_type": "code",
   "id": "3d499885",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:51:54.640245Z",
     "start_time": "2025-11-26T09:51:52.249806Z"
    }
   },
   "source": [
    "# Create a complete date range from train data\n",
    "date_range = pd.date_range(start=train['date'].min(), end=train['date'].max(), freq='D')\n",
    "\n",
    "# Get all unique combinations of store number and product family\n",
    "stores_list = train['store_nbr'].unique()\n",
    "unique_product_families = train['family'].unique()\n",
    "\n",
    "# Create a MultiIndex with all combinations of date, store_nbr, and family\n",
    "complete_index = pd.MultiIndex.from_tuples(\n",
    "    product(date_range, stores_list, unique_product_families),\n",
    "    names=['date', 'store_nbr', 'family']\n",
    ")\n",
    "\n",
    "# Create a complete dataframe\n",
    "complete_df = pd.DataFrame(index=complete_index).reset_index()\n",
    "\n",
    "# Merge with original train data\n",
    "train_complete = complete_df.merge(\n",
    "    train,\n",
    "    on=['date', 'store_nbr', 'family'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing values i.e. sales and onpromotion with 0\n",
    "train_complete['sales'] = train_complete['sales'].fillna(0)\n",
    "train_complete['onpromotion'] = train_complete['onpromotion'].fillna(0)\n",
    "\n",
    "# 4 missing dates, 54 stores, 33 product families i.e. 4 * 54 * 33 = 7128\n",
    "print(f\"Added records: {len(train_complete) - len(train)}\")\n",
    "\n",
    "train = train_complete"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added records: 7128\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "f6b9e5f4",
   "metadata": {},
   "source": [
    "### Check missing oil dates"
   ]
  },
  {
   "cell_type": "code",
   "id": "71bae50b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:51:57.306008Z",
     "start_time": "2025-11-26T09:51:57.293865Z"
    }
   },
   "source": [
    "# Get the date range from earliest to latest in train and test data\n",
    "date_range = pd.date_range(start=train['date'].min(), end=test['date'].max(), freq='D')\n",
    "\n",
    "# Get unique dates in oil data\n",
    "oil_dates = oil['date'].unique()\n",
    "\n",
    "# Find missing dates\n",
    "missing_dates = date_range.difference(pd.DatetimeIndex(oil_dates))\n",
    "\n",
    "print(f\"Total dates in range: {len(date_range)}\")\n",
    "print(f\"Unique dates in oil: {len(oil_dates)}\")\n",
    "print(f\"Missing dates: {len(missing_dates)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dates in range: 1704\n",
      "Unique dates in oil: 1218\n",
      "Missing dates: 486\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We are once again missing dates, which disrupts our continuous timeseries. We will use the same method to fix it.",
   "id": "375441b171bef850"
  },
  {
   "cell_type": "code",
   "id": "32564df9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:53:19.483738Z",
     "start_time": "2025-11-26T09:53:19.452498Z"
    }
   },
   "source": [
    "# Create complete date range for the data from train and test data\n",
    "date_range = pd.date_range(start=train['date'].min(), end=test['date'].max(), freq='D')\n",
    "\n",
    "# Create complete dataframe with all dates\n",
    "oil_complete = pd.DataFrame({'date': date_range})\n",
    "\n",
    "# Merge with original oil data\n",
    "oil_complete = oil_complete.merge(\n",
    "    oil, \n",
    "    on='date', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Forward fill (use last known price for missing dates)\n",
    "oil_complete['dcoilwtico'] = oil_complete['dcoilwtico'].ffill()\n",
    "\n",
    "# Backward fill for any remaining NaNs at the start\n",
    "oil_complete['dcoilwtico'] = oil_complete['dcoilwtico'].bfill()\n",
    "\n",
    "# Update oil dataframe\n",
    "print(f\"Number of records after filling missing oil data: {len(oil_complete)}\")\n",
    "print(f\"Added records: {len(oil_complete) - len(oil)}\")\n",
    "\n",
    "oil = oil_complete"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records after filling missing oil data: 1704\n",
      "Added records: 486\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "1dfcb5c5",
   "metadata": {},
   "source": [
    "### Check transactions"
   ]
  },
  {
   "cell_type": "code",
   "id": "52a01d57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:54:05.247200Z",
     "start_time": "2025-11-26T09:54:04.939127Z"
    }
   },
   "source": [
    "# Total transactions count\n",
    "total_transactions_count = len(train.groupby([\"date\", \"store_nbr\"])['sales'].sum())\n",
    "\n",
    "print(f\"Missing records from transactions: {total_transactions_count - len(transactions)}\")\n",
    "\n",
    "\n",
    "store_sales = train.groupby(['date', 'store_nbr'])['sales'].sum().reset_index()\n",
    "\n",
    "# Merge transactions with sales\n",
    "transactions_complete = transactions.merge(\n",
    "    store_sales,\n",
    "    on=['date', 'store_nbr'],\n",
    "    how='outer'\n",
    ").sort_values(by=['date', 'store_nbr'], ignore_index=True)\n",
    "\n",
    "# For dates where sales are 0, set transactions to 0\n",
    "transactions_complete.loc[transactions_complete[\"sales\"].eq(0), \"transactions\"] = 0\n",
    "\n",
    "# Interpolate missing transactions for each store\n",
    "transactions_complete['transactions'] = transactions_complete.groupby('store_nbr')['transactions'].transform(\n",
    "    lambda x: x.interpolate(method='linear')\n",
    ")\n",
    "\n",
    "# Round transactions to remove fractions from interpolation\n",
    "transactions_complete['transactions'] = transactions_complete['transactions'].round().astype(int)\n",
    "\n",
    "# Drop sales column\n",
    "transactions_complete = transactions_complete.drop('sales', axis=1)\n",
    "\n",
    "print(f\"Added transactions: {len(transactions_complete) - len(transactions)}\")\n",
    "\n",
    "transactions = transactions_complete"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing records from transactions: 7664\n",
      "Added transactions: 7664\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "7fa25219",
   "metadata": {},
   "source": [
    "### Holidays"
   ]
  },
  {
   "cell_type": "code",
   "id": "26c36448",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T10:08:31.644681Z",
     "start_time": "2025-11-26T10:08:31.598404Z"
    }
   },
   "source": [
    "holidays_events[\"description\"] = holidays_events.apply(\n",
    "    lambda x: x[\"description\"].strip().lower().replace(x[\"locale_name\"].lower(), \"\"), axis=1 # Remove names\n",
    ").apply(\n",
    "    lambda x: \"futbol\" if \"futbol\" in x else x # Only keep futbol\n",
    ").replace(r'\\b(de|del|puente|traslado|recupero)\\b', '', regex=True\n",
    ").replace(r'[+-]\\d+', '', regex=True # Remove digits with leading + or -\n",
    ").str.strip()\n",
    "\n",
    "holidays_events = holidays_events[holidays_events[\"type\"] != \"Transfer\"] # Transfer holidays are basically not actual holidays"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "80de5532",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T10:08:34.252038Z",
     "start_time": "2025-11-26T10:08:34.248221Z"
    }
   },
   "source": [
    "def check_holiday(row, holiday_events):\n",
    "    holiday_rows = holiday_events[holiday_events['date'] == row['date']]\n",
    "    \n",
    "    # There are no holidays\n",
    "    if holiday_rows.empty:\n",
    "        return 0\n",
    "    \n",
    "    # Check for National holiday\n",
    "    if not holiday_rows[holiday_rows['locale'] == 'National'].empty:\n",
    "        return 1\n",
    "    \n",
    "    # Check for Regional holiday (state must match)\n",
    "    regional_holidays = holiday_rows[holiday_rows['locale'] == 'Regional']\n",
    "    if not regional_holidays.empty:\n",
    "        if not regional_holidays[regional_holidays['locale_name'] == row['state']].empty:\n",
    "            return 1\n",
    "    \n",
    "    # Check for Local holiday (city must match)\n",
    "    local_holidays = holiday_rows[holiday_rows['locale'] == 'Local']\n",
    "    if not local_holidays.empty:\n",
    "        if not local_holidays[local_holidays['locale_name'] == row['city']].empty:\n",
    "            return 1\n",
    "\n",
    "    return 0"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "2a2e4393",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T10:12:27.640044Z",
     "start_time": "2025-11-26T10:08:37.232880Z"
    }
   },
   "source": [
    "data = pd.concat(\n",
    "    [train, test], axis=0, ignore_index=True\n",
    ").merge(\n",
    "    transactions, on=['date', 'store_nbr'], how='left'  \n",
    ").merge(\n",
    "    oil, on='date', how='left'\n",
    ").merge(\n",
    "    stores, on='store_nbr', how='left'\n",
    ")\n",
    "\n",
    "# This can be optimized by using a more efficient approach (e.g. merging)\n",
    "data[\"holiday\"] = data.apply(\n",
    "    lambda x: check_holiday(x, holidays_events), axis=1\n",
    ")"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 12\u001B[39m\n\u001B[32m      1\u001B[39m data = pd.concat(\n\u001B[32m      2\u001B[39m     [train, test], axis=\u001B[32m0\u001B[39m, ignore_index=\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m      3\u001B[39m ).merge(\n\u001B[32m   (...)\u001B[39m\u001B[32m      8\u001B[39m     stores, on=\u001B[33m'\u001B[39m\u001B[33mstore_nbr\u001B[39m\u001B[33m'\u001B[39m, how=\u001B[33m'\u001B[39m\u001B[33mleft\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m      9\u001B[39m )\n\u001B[32m     11\u001B[39m \u001B[38;5;66;03m# This can be optimized by using a more efficient approach (e.g. merging)\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m data[\u001B[33m\"\u001B[39m\u001B[33mholiday\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_holiday\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mholidays_events\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\n\u001B[32m     14\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/pandas/core/frame.py:10381\u001B[39m, in \u001B[36mDataFrame.apply\u001B[39m\u001B[34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001B[39m\n\u001B[32m  10367\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcore\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mapply\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m frame_apply\n\u001B[32m  10369\u001B[39m op = frame_apply(\n\u001B[32m  10370\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m  10371\u001B[39m     func=func,\n\u001B[32m   (...)\u001B[39m\u001B[32m  10379\u001B[39m     kwargs=kwargs,\n\u001B[32m  10380\u001B[39m )\n\u001B[32m> \u001B[39m\u001B[32m10381\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mop\u001B[49m\u001B[43m.\u001B[49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m.__finalize__(\u001B[38;5;28mself\u001B[39m, method=\u001B[33m\"\u001B[39m\u001B[33mapply\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/pandas/core/apply.py:916\u001B[39m, in \u001B[36mFrameApply.apply\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    913\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.raw:\n\u001B[32m    914\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.apply_raw(engine=\u001B[38;5;28mself\u001B[39m.engine, engine_kwargs=\u001B[38;5;28mself\u001B[39m.engine_kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m916\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/pandas/core/apply.py:1063\u001B[39m, in \u001B[36mFrameApply.apply_standard\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1061\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mapply_standard\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m   1062\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.engine == \u001B[33m\"\u001B[39m\u001B[33mpython\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m1063\u001B[39m         results, res_index = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mapply_series_generator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1064\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1065\u001B[39m         results, res_index = \u001B[38;5;28mself\u001B[39m.apply_series_numba()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/pandas/core/apply.py:1081\u001B[39m, in \u001B[36mFrameApply.apply_series_generator\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1078\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m option_context(\u001B[33m\"\u001B[39m\u001B[33mmode.chained_assignment\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m   1079\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m i, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(series_gen):\n\u001B[32m   1080\u001B[39m         \u001B[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1081\u001B[39m         results[i] = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1082\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(results[i], ABCSeries):\n\u001B[32m   1083\u001B[39m             \u001B[38;5;66;03m# If we have a view on v, we need to make a copy because\u001B[39;00m\n\u001B[32m   1084\u001B[39m             \u001B[38;5;66;03m#  series_generator will swap out the underlying data\u001B[39;00m\n\u001B[32m   1085\u001B[39m             results[i] = results[i].copy(deep=\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 13\u001B[39m, in \u001B[36m<lambda>\u001B[39m\u001B[34m(x)\u001B[39m\n\u001B[32m      1\u001B[39m data = pd.concat(\n\u001B[32m      2\u001B[39m     [train, test], axis=\u001B[32m0\u001B[39m, ignore_index=\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m      3\u001B[39m ).merge(\n\u001B[32m   (...)\u001B[39m\u001B[32m      8\u001B[39m     stores, on=\u001B[33m'\u001B[39m\u001B[33mstore_nbr\u001B[39m\u001B[33m'\u001B[39m, how=\u001B[33m'\u001B[39m\u001B[33mleft\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m      9\u001B[39m )\n\u001B[32m     11\u001B[39m \u001B[38;5;66;03m# This can be optimized by using a more efficient approach (e.g. merging)\u001B[39;00m\n\u001B[32m     12\u001B[39m data[\u001B[33m\"\u001B[39m\u001B[33mholiday\u001B[39m\u001B[33m\"\u001B[39m] = data.apply(\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m     \u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[43mcheck_holiday\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mholidays_events\u001B[49m\u001B[43m)\u001B[49m, axis=\u001B[32m1\u001B[39m\n\u001B[32m     14\u001B[39m )\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 9\u001B[39m, in \u001B[36mcheck_holiday\u001B[39m\u001B[34m(row, holiday_events)\u001B[39m\n\u001B[32m      6\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[32m0\u001B[39m\n\u001B[32m      8\u001B[39m \u001B[38;5;66;03m# Check for National holiday\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m holiday_rows[\u001B[43mholiday_rows\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mlocale\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m == \u001B[33m'\u001B[39m\u001B[33mNational\u001B[39m\u001B[33m'\u001B[39m].empty:\n\u001B[32m     10\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[32m1\u001B[39m\n\u001B[32m     12\u001B[39m \u001B[38;5;66;03m# Check for Regional holiday (state must match)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/pandas/core/frame.py:4083\u001B[39m, in \u001B[36mDataFrame.__getitem__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   4076\u001B[39m \u001B[38;5;66;03m# GH#45316 Return view if key is not duplicated\u001B[39;00m\n\u001B[32m   4077\u001B[39m \u001B[38;5;66;03m# Only use drop_duplicates with duplicates for performance\u001B[39;00m\n\u001B[32m   4078\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_mi \u001B[38;5;129;01mand\u001B[39;00m (\n\u001B[32m   4079\u001B[39m     \u001B[38;5;28mself\u001B[39m.columns.is_unique\n\u001B[32m   4080\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.columns\n\u001B[32m   4081\u001B[39m     \u001B[38;5;129;01mor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.columns.drop_duplicates(keep=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m   4082\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m4083\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_item_cache\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4085\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m is_mi \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.columns.is_unique \u001B[38;5;129;01mand\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.columns:\n\u001B[32m   4086\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._getitem_multilevel(key)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/pandas/core/frame.py:4644\u001B[39m, in \u001B[36mDataFrame._get_item_cache\u001B[39m\u001B[34m(self, item)\u001B[39m\n\u001B[32m   4639\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m res \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   4640\u001B[39m     \u001B[38;5;66;03m# All places that call _get_item_cache have unique columns,\u001B[39;00m\n\u001B[32m   4641\u001B[39m     \u001B[38;5;66;03m#  pending resolution of GH#33047\u001B[39;00m\n\u001B[32m   4643\u001B[39m     loc = \u001B[38;5;28mself\u001B[39m.columns.get_loc(item)\n\u001B[32m-> \u001B[39m\u001B[32m4644\u001B[39m     res = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_ixs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   4646\u001B[39m     cache[item] = res\n\u001B[32m   4648\u001B[39m     \u001B[38;5;66;03m# for a chain\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/pandas/core/frame.py:4015\u001B[39m, in \u001B[36mDataFrame._ixs\u001B[39m\u001B[34m(self, i, axis)\u001B[39m\n\u001B[32m   4011\u001B[39m \u001B[38;5;66;03m# icol\u001B[39;00m\n\u001B[32m   4012\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   4013\u001B[39m     label = \u001B[38;5;28mself\u001B[39m.columns[i]\n\u001B[32m-> \u001B[39m\u001B[32m4015\u001B[39m     col_mgr = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_mgr\u001B[49m\u001B[43m.\u001B[49m\u001B[43miget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4016\u001B[39m     result = \u001B[38;5;28mself\u001B[39m._box_col_values(col_mgr, i)\n\u001B[32m   4018\u001B[39m     \u001B[38;5;66;03m# this is a cached value, mark it so\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/pandas/core/internals/managers.py:1017\u001B[39m, in \u001B[36mBlockManager.iget\u001B[39m\u001B[34m(self, i, track_ref)\u001B[39m\n\u001B[32m   1013\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1014\u001B[39m \u001B[33;03mReturn the data as a SingleBlockManager.\u001B[39;00m\n\u001B[32m   1015\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1016\u001B[39m block = \u001B[38;5;28mself\u001B[39m.blocks[\u001B[38;5;28mself\u001B[39m.blknos[i]]\n\u001B[32m-> \u001B[39m\u001B[32m1017\u001B[39m values = block.iget(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mblklocs\u001B[49m[i])\n\u001B[32m   1019\u001B[39m \u001B[38;5;66;03m# shortcut for select a single-dim from a 2-dim BM\u001B[39;00m\n\u001B[32m   1020\u001B[39m bp = BlockPlacement(\u001B[38;5;28mslice\u001B[39m(\u001B[32m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(values)))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/pandas/core/internals/managers.py:196\u001B[39m, in \u001B[36mBaseBlockManager.blklocs\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    192\u001B[39m         \u001B[38;5;28mself\u001B[39m._rebuild_blknos_and_blklocs()\n\u001B[32m    194\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._blknos\n\u001B[32m--> \u001B[39m\u001B[32m196\u001B[39m \u001B[38;5;129m@property\u001B[39m\n\u001B[32m    197\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mblklocs\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> npt.NDArray[np.intp]:\n\u001B[32m    198\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    199\u001B[39m \u001B[33;03m    See blknos.__doc__\u001B[39;00m\n\u001B[32m    200\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m    201\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._blklocs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    202\u001B[39m         \u001B[38;5;66;03m# Note: these can be altered by other BlockManager methods.\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "2e5c9a14",
   "metadata": {},
   "source": [
    "### Creating one and two day lags for dcoilwtico"
   ]
  },
  {
   "cell_type": "code",
   "id": "f42a9765",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T10:13:18.414916Z",
     "start_time": "2025-11-26T10:13:16.771500Z"
    }
   },
   "source": [
    "# Sort by store_nbr, family, and date to ensure correct order within each group\n",
    "data = data.sort_values(['store_nbr', 'family', 'date'])\n",
    "\n",
    "# Create lag-1 column within each store-family combination\n",
    "data['lag-1-dcoilwtico'] = data.groupby(['store_nbr', 'family'])['dcoilwtico'].shift(1)\n",
    "\n",
    "# Create lag-2 column (2 days ago)\n",
    "data['lag-2-dcoilwtico'] = data.groupby(['store_nbr', 'family'])['dcoilwtico'].shift(2)\n",
    "\n",
    "# Backfill NaNs at the start of each group\n",
    "data['lag-1-dcoilwtico'] = data.groupby(['store_nbr', 'family'])['lag-1-dcoilwtico'].bfill()\n",
    "data['lag-2-dcoilwtico'] = data.groupby(['store_nbr', 'family'])['lag-2-dcoilwtico'].bfill()\n",
    "data = data.sort_values('date')"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "eab82a7a",
   "metadata": {},
   "source": [
    "### Extract date features"
   ]
  },
  {
   "cell_type": "code",
   "id": "32ab3675",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T10:14:01.577135Z",
     "start_time": "2025-11-26T10:14:01.123615Z"
    }
   },
   "source": [
    "data['year'] = data['date'].dt.year\n",
    "data['month'] = data['date'].dt.month\n",
    "data['day'] = data['date'].dt.day\n",
    "data['day_of_week'] = data['date'].dt.dayofweek\n",
    "data['day_of_year'] = data['date'].dt.dayofyear\n",
    "data['week_of_year'] = data['date'].dt.isocalendar().week\n",
    "data['is_weekend'] = (data['date'].dt.dayofweek >= 5).astype(int)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "96f579f3",
   "metadata": {},
   "source": [
    "### Label Encoding for categorical features"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Since most ML models cannot work with categorical features like \"AUTOMOTIVE\" etc, we have to numerically encode them for the model to work.",
   "id": "81fbb111f774a079"
  },
  {
   "cell_type": "code",
   "id": "7e9bbb6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T10:14:36.099630Z",
     "start_time": "2025-11-26T10:14:34.848712Z"
    }
   },
   "source": [
    "# Get all unique values for each categorical column from data\n",
    "all_product_families = data['family'].unique()\n",
    "all_cities = data['city'].unique()\n",
    "all_states = data['state'].unique()\n",
    "all_types = data['type'].unique()\n",
    "\n",
    "# Initialize label encoders\n",
    "le_product_family = LabelEncoder()\n",
    "le_city = LabelEncoder()\n",
    "le_state = LabelEncoder()\n",
    "le_type = LabelEncoder()\n",
    "\n",
    "# Fit on all unique values\n",
    "le_product_family.fit(all_product_families)\n",
    "le_city.fit(all_cities)\n",
    "le_state.fit(all_states)\n",
    "le_type.fit(all_types)\n",
    "\n",
    "# Transform data dataframe\n",
    "data['family'] = le_product_family.transform(data['family'])\n",
    "data['city'] = le_city.transform(data['city'])\n",
    "data['state'] = le_state.transform(data['state'])\n",
    "data['type'] = le_type.transform(data['type'])\n",
    "\n",
    "print(\"Label encoding complete!\")\n",
    "print(f\"Family: {len(all_product_families)} categories\")\n",
    "print(f\"City: {len(all_cities)} categories\")\n",
    "print(f\"State: {len(all_states)} categories\")\n",
    "print(f\"Type: {len(all_types)} categories\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoding complete!\n",
      "Family: 33 categories\n",
      "City: 22 categories\n",
      "State: 16 categories\n",
      "Type: 5 categories\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "56ae8e71",
   "metadata": {},
   "source": [
    "### Save final version of 'data' dataframe "
   ]
  },
  {
   "cell_type": "code",
   "id": "8e90d795",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T10:20:11.237790Z",
     "start_time": "2025-11-26T10:19:54.933423Z"
    }
   },
   "source": [
    "data.to_csv('./data/data.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "5fb22739",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67725cad",
   "metadata": {},
   "source": [
    "### Splitting test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6ca108d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (3008016, 19)\n",
      "Test shape: (28512, 18)\n",
      "Test IDs saved: 28512\n"
     ]
    }
   ],
   "source": [
    "# Split based on date\n",
    "train = data[data['date'] < '2017-08-16'].copy()\n",
    "test = data[data['date'] >= '2017-08-16'].copy()\n",
    "\n",
    "# Save test IDs before dropping\n",
    "test_ids = test['id'].copy()\n",
    "\n",
    "# Drop transactions column from both\n",
    "train = train.drop('transactions', axis=1)\n",
    "test = test.drop('transactions', axis=1)\n",
    "\n",
    "# Drop date column since we have date features\n",
    "train = train.drop('date', axis=1)\n",
    "test = test.drop('date', axis=1)\n",
    "\n",
    "# Drop id column\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)\n",
    "\n",
    "# Drop sales column only from test\n",
    "test = test.drop('sales', axis=1)\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"Test IDs saved: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d12a9368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (3008016, 18)\n",
      "Test set: (28512, 18)\n",
      "Target: (3008016,)\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X_train = train.drop('sales', axis=1)\n",
    "y_train = train['sales']\n",
    "\n",
    "X_test = test \n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Target: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc8877c",
   "metadata": {},
   "source": [
    "### Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ced950f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest_model(X_train, y_train):\n",
    "    # Initialize Random Forest model\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=20,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=4,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training Random Forest model...\")\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on training set to evaluate\n",
    "    y_train_pred_rf = np.maximum(0, rf_model.predict(X_train))\n",
    "\n",
    "    # Calculate metrics\n",
    "    rmsle_score = np.sqrt(mean_squared_log_error(y_train, y_train_pred_rf))\n",
    "\n",
    "    return rf_model, rmsle_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781d3f5f",
   "metadata": {},
   "source": [
    "### LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b9bdd511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lightgbm_model(X_train, y_train):\n",
    "    # Initialize LightGBM model\n",
    "    lgb_model = LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=20,\n",
    "        learning_rate=0.1,\n",
    "        num_leaves=31,\n",
    "        min_child_samples=20,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training LightGBM model...\")\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on training set to evaluate\n",
    "    y_train_pred_lgb = np.maximum(0, lgb_model.predict(X_train))\n",
    "\n",
    "    # Calculate metrics\n",
    "    rmsle_score_lgb = np.sqrt(mean_squared_log_error(y_train, y_train_pred_lgb))\n",
    "\n",
    "    return lgb_model, rmsle_score_lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05df1d02",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4270622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_model(X_train, y_train):\n",
    "    # Initialize XGBoost model\n",
    "    xgb_model = XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        learning_rate=0.1,\n",
    "        min_child_weight=3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbosity=1\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training XGBoost model...\")\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on training set to evaluate\n",
    "    y_train_pred_xgb = np.maximum(0, xgb_model.predict(X_train))\n",
    "\n",
    "    # Calculate metrics\n",
    "    rmsle_score_xgb = np.sqrt(mean_squared_log_error(y_train, y_train_pred_xgb))\n",
    "\n",
    "    return xgb_model, rmsle_score_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3c7bf7",
   "metadata": {},
   "source": [
    "### Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dbfa36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(predictions, test_ids, filename='submission.csv'):\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_ids.values.astype(int),\n",
    "        'sales': predictions\n",
    "    })\n",
    "    \n",
    "    submission.to_csv(filename, index=False)\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbbd718",
   "metadata": {},
   "source": [
    "### Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model, xgb_rmsle_score = train_xgboost_model(X_train, y_train)\n",
    "rf_model, rf_rmsle_score = train_random_forest_model(X_train, y_train)\n",
    "lgbm_model, lgbm_rmsle_score = train_lightgbm_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54eba17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance Comparison:\n",
      "        Model    RMSLE\n",
      "Random Forest 0.625777\n",
      "     LightGBM 2.154993\n",
      "      XGBoost 1.445509\n",
      "\n",
      "Best model by RMSLE: Random Forest\n"
     ]
    }
   ],
   "source": [
    "# Compare all models including ensemble\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'LightGBM', 'XGBoost'],\n",
    "    'RMSLE': [rf_rmsle_score, lgbm_rmsle_score, xgb_rmsle_score]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(f\"\\nBest model by RMSLE: {comparison_df.loc[comparison_df['RMSLE'].idxmin(), 'Model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e3cea4",
   "metadata": {},
   "source": [
    "### Create submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7987718b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All submission files created successfully!\n"
     ]
    }
   ],
   "source": [
    "xgb_y_pred = xgb_model.predict(X_test)\n",
    "rf_y_pred = rf_model.predict(X_test)\n",
    "lgbm_y_pred = lgbm_model.predict(X_test)\n",
    "\n",
    "# Create submission files using the saved test IDs\n",
    "xgb_submission = create_submission(xgb_y_pred, test_ids, 'xgb_submission.csv')\n",
    "rf_submission = create_submission(rf_y_pred, test_ids, 'rf_submission.csv')\n",
    "lgbm_submission = create_submission(lgbm_y_pred, test_ids, 'lgbm_submission.csv')\n",
    "\n",
    "print(\"All submission files created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
