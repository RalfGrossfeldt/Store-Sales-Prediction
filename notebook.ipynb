{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5315f7cd",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6c93681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import mean_squared_log_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d662c64b",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f03f96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv', parse_dates=['date'])\n",
    "test = pd.read_csv('./data/test.csv', parse_dates=['date'])\n",
    "holidays_events = pd.read_csv('./data/holidays_events.csv', parse_dates=['date'])\n",
    "oil = pd.read_csv('./data/oil.csv', parse_dates=['date'])\n",
    "stores = pd.read_csv('./data/stores.csv')\n",
    "transactions = pd.read_csv('./data/transactions.csv', parse_dates=['date'])\n",
    "data = pd.read_csv('./data/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31142406",
   "metadata": {},
   "source": [
    "### Check if there are missing dates in the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef8903c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dates in range: 1688\n",
      "Unique dates in train: 1684\n",
      "Missing dates: 4\n",
      "\n",
      "Missing dates:\n",
      "DatetimeIndex(['2013-12-25', '2014-12-25', '2015-12-25', '2016-12-25'], dtype='datetime64[ns]', freq=None)\n"
     ]
    }
   ],
   "source": [
    "# Get the date range from earliest to latest in train data\n",
    "date_range = pd.date_range(start=train['date'].min(), end=train['date'].max(), freq='D')\n",
    "\n",
    "# Get unique dates in train data\n",
    "train_dates = train['date'].unique()\n",
    "\n",
    "# Find missing dates\n",
    "missing_dates = date_range.difference(pd.DatetimeIndex(train_dates))\n",
    "\n",
    "print(f\"Total dates in range: {len(date_range)}\")\n",
    "print(f\"Unique dates in train: {len(train_dates)}\")\n",
    "print(f\"Missing dates: {len(missing_dates)}\")\n",
    "print(f\"\\nMissing dates:\\n{missing_dates}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d499885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added records: 7128\n"
     ]
    }
   ],
   "source": [
    "# Create a complete date range from train data\n",
    "date_range = pd.date_range(start=train['date'].min(), end=train['date'].max(), freq='D')\n",
    "\n",
    "# Get all unique combinations of store number and product family\n",
    "stores_list = train['store_nbr'].unique()\n",
    "unique_product_families = train['family'].unique()\n",
    "\n",
    "# Create a MultiIndex with all combinations of date, store_nbr, and family\n",
    "complete_index = pd.MultiIndex.from_tuples(\n",
    "    product(date_range, stores_list, unique_product_families),\n",
    "    names=['date', 'store_nbr', 'family']\n",
    ")\n",
    "\n",
    "# Create a complete dataframe\n",
    "complete_df = pd.DataFrame(index=complete_index).reset_index()\n",
    "\n",
    "# Merge with original train data\n",
    "train_complete = complete_df.merge(\n",
    "    train,\n",
    "    on=['date', 'store_nbr', 'family'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing values i.e. sales and onpromotion with 0\n",
    "train_complete['sales'] = train_complete['sales'].fillna(0)\n",
    "train_complete['onpromotion'] = train_complete['onpromotion'].fillna(0)\n",
    "\n",
    "# 4 missing dates, 54 stores, 33 product families i.e. 4 * 54 * 33 = 7128\n",
    "print(f\"Added records: {len(train_complete) - len(train)}\")\n",
    "\n",
    "train = train_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b9e5f4",
   "metadata": {},
   "source": [
    "### Check missing oil dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71bae50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dates in range: 1704\n",
      "Unique dates in oil: 1218\n",
      "Missing dates: 486\n"
     ]
    }
   ],
   "source": [
    "# Get the date range from earliest to latest in train and test data\n",
    "date_range = pd.date_range(start=train['date'].min(), end=test['date'].max(), freq='D')\n",
    "\n",
    "# Get unique dates in oil data\n",
    "oil_dates = oil['date'].unique()\n",
    "\n",
    "# Find missing dates\n",
    "missing_dates = date_range.difference(pd.DatetimeIndex(oil_dates))\n",
    "\n",
    "print(f\"Total dates in range: {len(date_range)}\")\n",
    "print(f\"Unique dates in oil: {len(oil_dates)}\")\n",
    "print(f\"Missing dates: {len(missing_dates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32564df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records after filling missing oil data: 1704\n",
      "Added records: 486\n"
     ]
    }
   ],
   "source": [
    "# Create complete date range for the data from train and test data\n",
    "date_range = pd.date_range(start=train['date'].min(), end=test['date'].max(), freq='D')\n",
    "\n",
    "# Create complete dataframe with all dates\n",
    "oil_complete = pd.DataFrame({'date': date_range})\n",
    "\n",
    "# Merge with original oil data\n",
    "oil_complete = oil_complete.merge(\n",
    "    oil, \n",
    "    on='date', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Forward fill (use last known price for missing dates)\n",
    "oil_complete['dcoilwtico'] = oil_complete['dcoilwtico'].ffill()\n",
    "\n",
    "# Backward fill for any remaining NaNs at the start\n",
    "oil_complete['dcoilwtico'] = oil_complete['dcoilwtico'].bfill()\n",
    "\n",
    "# Update oil dataframe\n",
    "print(f\"Number of records after filling missing oil data: {len(oil_complete)}\")\n",
    "print(f\"Added records: {len(oil_complete) - len(oil)}\")\n",
    "\n",
    "oil = oil_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfcb5c5",
   "metadata": {},
   "source": [
    "### Check transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52a01d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing records from transactions: 7664\n",
      "Added transactions: 7664\n"
     ]
    }
   ],
   "source": [
    "# Total transactions count\n",
    "total_transactions_count = len(train.groupby([\"date\", \"store_nbr\"])['sales'].sum())\n",
    "\n",
    "print(f\"Missing records from transactions: {total_transactions_count - len(transactions)}\")\n",
    "\n",
    "\n",
    "store_sales = train.groupby(['date', 'store_nbr'])['sales'].sum().reset_index()\n",
    "\n",
    "# Merge transactions with sales\n",
    "transactions_complete = transactions.merge(\n",
    "    store_sales,\n",
    "    on=['date', 'store_nbr'],\n",
    "    how='outer'\n",
    ").sort_values(by=['date', 'store_nbr'], ignore_index=True)\n",
    "\n",
    "# For dates where sales are 0, set transactions to 0\n",
    "transactions_complete.loc[transactions_complete[\"sales\"].eq(0), \"transactions\"] = 0\n",
    "\n",
    "# Interpolate missing transactions for each store\n",
    "transactions_complete['transactions'] = transactions_complete.groupby('store_nbr')['transactions'].transform(\n",
    "    lambda x: x.interpolate(method='linear')\n",
    ")\n",
    "\n",
    "# Round transactions to remove fractions from interpolation\n",
    "transactions_complete['transactions'] = transactions_complete['transactions'].round().astype(int)\n",
    "\n",
    "# Drop sales column\n",
    "transactions_complete = transactions_complete.drop('sales', axis=1)\n",
    "\n",
    "print(f\"Added transactions: {len(transactions_complete) - len(transactions)}\")\n",
    "\n",
    "transactions = transactions_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa25219",
   "metadata": {},
   "source": [
    "### Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "26c36448",
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_events[\"description\"] = holidays_events.apply(\n",
    "    lambda x: x[\"description\"].strip().lower().replace(x[\"locale_name\"].lower(), \"\"), axis=1 # Remove names\n",
    ").apply(\n",
    "    lambda x: \"futbol\" if \"futbol\" in x else x # Only keep futbol\n",
    ").replace(r'\\b(de|del|puente|traslado|recupero)\\b', '', regex=True\n",
    ").replace(r'[+-]\\d+', '', regex=True # Remove digits with leading + or -\n",
    ").str.strip()\n",
    "\n",
    "holidays_events = holidays_events[holidays_events[\"type\"] != \"Transfer\"] # Transfer holidays are basically not actual holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80de5532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_holiday(row, holiday_events):\n",
    "    holiday_rows = holiday_events[holiday_events['date'] == row['date']]\n",
    "    \n",
    "    # There are no holidays\n",
    "    if holiday_rows.empty:\n",
    "        return 0\n",
    "    \n",
    "    # Check for National holiday\n",
    "    if not holiday_rows[holiday_rows['locale'] == 'National'].empty:\n",
    "        return 1\n",
    "    \n",
    "    # Check for Regional holiday (state must match)\n",
    "    regional_holidays = holiday_rows[holiday_rows['locale'] == 'Regional']\n",
    "    if not regional_holidays.empty:\n",
    "        if not regional_holidays[regional_holidays['locale_name'] == row['state']].empty:\n",
    "            return 1\n",
    "    \n",
    "    # Check for Local holiday (city must match)\n",
    "    local_holidays = holiday_rows[holiday_rows['locale'] == 'Local']\n",
    "    if not local_holidays.empty:\n",
    "        if not local_holidays[local_holidays['locale_name'] == row['city']].empty:\n",
    "            return 1\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e4393",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat(\n",
    "    [train, test], axis=0, ignore_index=True\n",
    ").merge(\n",
    "    transactions, on=['date', 'store_nbr'], how='left'  \n",
    ").merge(\n",
    "    oil, on='date', how='left'\n",
    ").merge(\n",
    "    stores, on='store_nbr', how='left'\n",
    ")\n",
    "\n",
    "# This can be optimized by using a more efficient approach (e.g. merging)\n",
    "data[\"holiday\"] = data.apply(\n",
    "    lambda x: check_holiday(x, holidays_events), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5c9a14",
   "metadata": {},
   "source": [
    "### Creating one and two day lags for dcoilwtico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f42a9765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by store_nbr, family, and date to ensure correct order within each group\n",
    "data = data.sort_values(['store_nbr', 'family', 'date'])\n",
    "\n",
    "# Create lag-1 column within each store-family combination\n",
    "data['lag-1-dcoilwtico'] = data.groupby(['store_nbr', 'family'])['dcoilwtico'].shift(1)\n",
    "\n",
    "# Create lag-2 column (2 days ago)\n",
    "data['lag-2-dcoilwtico'] = data.groupby(['store_nbr', 'family'])['dcoilwtico'].shift(2)\n",
    "\n",
    "# Backfill NaNs at the start of each group\n",
    "data['lag-1-dcoilwtico'] = data.groupby(['store_nbr', 'family'])['lag-1-dcoilwtico'].bfill()\n",
    "data['lag-2-dcoilwtico'] = data.groupby(['store_nbr', 'family'])['lag-2-dcoilwtico'].bfill()\n",
    "data = data.sort_values('date')\n",
    "\n",
    "data.to_csv('./data/data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab82a7a",
   "metadata": {},
   "source": [
    "### Extract date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "32ab3675",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['year'] = data['date'].dt.year\n",
    "data['month'] = data['date'].dt.month\n",
    "data['day'] = data['date'].dt.day\n",
    "data['day_of_week'] = data['date'].dt.dayofweek\n",
    "data['day_of_year'] = data['date'].dt.dayofyear\n",
    "data['week_of_year'] = data['date'].dt.isocalendar().week\n",
    "data['is_weekend'] = (data['date'].dt.dayofweek >= 5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f579f3",
   "metadata": {},
   "source": [
    "### Label Encoding for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e9bbb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoding complete!\n",
      "Family: 33 categories\n",
      "City: 22 categories\n",
      "State: 16 categories\n",
      "Type: 5 categories\n"
     ]
    }
   ],
   "source": [
    "# Get all unique values for each categorical column from data\n",
    "all_product_families = data['family'].unique()\n",
    "all_cities = data['city'].unique()\n",
    "all_states = data['state'].unique()\n",
    "all_types = data['type'].unique()\n",
    "\n",
    "# Initialize label encoders\n",
    "le_product_family = LabelEncoder()\n",
    "le_city = LabelEncoder()\n",
    "le_state = LabelEncoder()\n",
    "le_type = LabelEncoder()\n",
    "\n",
    "# Fit on all unique values\n",
    "le_product_family.fit(all_product_families)\n",
    "le_city.fit(all_cities)\n",
    "le_state.fit(all_states)\n",
    "le_type.fit(all_types)\n",
    "\n",
    "# Transform data dataframe\n",
    "data['family'] = le_product_family.transform(data['family'])\n",
    "data['city'] = le_city.transform(data['city'])\n",
    "data['state'] = le_state.transform(data['state'])\n",
    "data['type'] = le_type.transform(data['type'])\n",
    "\n",
    "print(\"Label encoding complete!\")\n",
    "print(f\"Family: {len(all_product_families)} categories\")\n",
    "print(f\"City: {len(all_cities)} categories\")\n",
    "print(f\"State: {len(all_states)} categories\")\n",
    "print(f\"Type: {len(all_types)} categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67725cad",
   "metadata": {},
   "source": [
    "### Splitting test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6ca108d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (3008016, 19)\n",
      "Test shape: (28512, 18)\n",
      "Test IDs saved: 28512\n"
     ]
    }
   ],
   "source": [
    "# Split based on date\n",
    "train = data[data['date'] < '2017-08-16'].copy()\n",
    "test = data[data['date'] >= '2017-08-16'].copy()\n",
    "\n",
    "# Save test IDs before dropping\n",
    "test_ids = test['id'].copy()\n",
    "\n",
    "# Drop transactions column from both\n",
    "train = train.drop('transactions', axis=1)\n",
    "test = test.drop('transactions', axis=1)\n",
    "\n",
    "# Drop date column since we have date features\n",
    "train = train.drop('date', axis=1)\n",
    "test = test.drop('date', axis=1)\n",
    "\n",
    "# Drop id column\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)\n",
    "\n",
    "# Drop sales column only from test\n",
    "test = test.drop('sales', axis=1)\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"Test IDs saved: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb22739",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a9368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (3008016, 18)\n",
      "Test set: (28512, 18)\n",
      "Target: (3008016,)\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X_train = train.drop('sales', axis=1)\n",
    "y_train = train['sales']\n",
    "\n",
    "X_test = test \n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Target: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ced950f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 11.2min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 11.2min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    7.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "RMSE: 195.05\n",
      "MAE: 40.76\n",
      "R² Score: 0.9686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:   19.2s finished\n"
     ]
    }
   ],
   "source": [
    "# Initialize Random Forest model\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=4,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Random Forest model...\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on training set to evaluate\n",
    "y_train_pred = rf_model.predict(X_train)\n",
    "\n",
    "# Calculate metrics\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "rmsle_score = np.sqrt(mean_squared_log_error(y_train, y_train_pred))\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"\\nTraining Metrics:\")\n",
    "print(f\"RMSLE: {rmsle_score:.2f}\")\n",
    "print(f\"RMSE: {train_rmse:.2f}\")\n",
    "print(f\"MAE: {train_mae:.2f}\")\n",
    "print(f\"R² Score: {train_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cff3bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions generated: 28512\n",
      "Sample predictions: [7.64449679e-01 1.46636656e+02 3.17176471e-01 1.15108877e+00\n",
      " 2.18462200e+02 3.84661752e+03 9.27108390e+03 1.37646594e+02\n",
      " 3.09251844e+03 4.57692364e+02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test set\n",
    "y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "print(f\"Test predictions generated: {len(y_test_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd98f9a",
   "metadata": {},
   "source": [
    "# Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbfa36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(predictions, test_ids, filename='submission.csv'):\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_ids.values.astype(int),\n",
    "        'sales': predictions\n",
    "    })\n",
    "    \n",
    "    submission.to_csv(filename, index=False)\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7987718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file using the saved test IDs\n",
    "submission = create_submission(y_test_pred, test_ids, 'submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
