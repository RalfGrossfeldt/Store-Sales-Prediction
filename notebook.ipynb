{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e2df117",
   "metadata": {},
   "source": [
    "# pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed54e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lightgbm scikit-learn xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5315f7cd",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "6c93681c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:51:39.513170Z",
     "start_time": "2025-11-26T09:51:37.921221Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data preparation\n",
    "We will check for missing data, numerically encode categorical variables, and do some feature engineering in regards to potential oil price lags."
   ],
   "id": "d662c64b"
  },
  {
   "cell_type": "code",
   "id": "4f03f96c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:51:45.090367Z",
     "start_time": "2025-11-26T09:51:43.997419Z"
    }
   },
   "source": [
    "# Notebook location\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "# Data directory\n",
    "DATA_DIR = NOTEBOOK_DIR / \"data\"\n",
    "\n",
    "#parse_dates=['date'] converts date values into datetime objects\n",
    "train = pd.read_csv(DATA_DIR / \"train.csv\", parse_dates=['date'])\n",
    "test = pd.read_csv(DATA_DIR / 'test.csv', parse_dates=['date'])\n",
    "holidays_events = pd.read_csv(DATA_DIR / 'holidays_events.csv', parse_dates=['date'])\n",
    "oil = pd.read_csv(DATA_DIR / 'oil.csv', parse_dates=['date'])\n",
    "stores = pd.read_csv(DATA_DIR / 'stores.csv')\n",
    "transactions = pd.read_csv(DATA_DIR / 'transactions.csv', parse_dates=['date'])\n",
    "#data = pd.read_csv(DATA_DIR / 'data.csv', parse_dates=['date'])"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "31142406",
   "metadata": {},
   "source": [
    "### Check if there are missing dates in the train data"
   ]
  },
  {
   "cell_type": "code",
   "id": "ef8903c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:51:47.366878Z",
     "start_time": "2025-11-26T09:51:47.340539Z"
    }
   },
   "source": [
    "# Get the date range from earliest to latest in train data\n",
    "date_range = pd.date_range(start=train['date'].min(), end=train['date'].max(), freq='D')\n",
    "\n",
    "# Get unique dates in train data\n",
    "train_dates = train['date'].unique()\n",
    "\n",
    "# Find missing dates\n",
    "missing_dates = date_range.difference(pd.DatetimeIndex(train_dates))\n",
    "\n",
    "print(f\"Total dates in range: {len(date_range)}\")\n",
    "print(f\"Unique dates in train: {len(train_dates)}\")\n",
    "print(f\"Missing dates: {len(missing_dates)}\")\n",
    "print(f\"\\nMissing dates:\\n{missing_dates}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dates in range: 1688\n",
      "Unique dates in train: 1684\n",
      "Missing dates: 4\n",
      "\n",
      "Missing dates:\n",
      "DatetimeIndex(['2013-12-25', '2014-12-25', '2015-12-25', '2016-12-25'], dtype='datetime64[ns]', freq=None)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have 4 missing dates in total. This will cause problems in modelling, since timeseries models require a continuous flow time. Forecasting models require need the timeseries input to be complete, not missing any dates.  The following code will fix this and set the target value of sales as 0 on these days.",
   "id": "8f3b2f7ed717a3e6"
  },
  {
   "cell_type": "code",
   "id": "3d499885",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:51:54.640245Z",
     "start_time": "2025-11-26T09:51:52.249806Z"
    }
   },
   "source": [
    "# Create a complete date range from train data\n",
    "date_range = pd.date_range(start=train['date'].min(), end=train['date'].max(), freq='D')\n",
    "\n",
    "# Get all unique combinations of store number and product family\n",
    "stores_list = train['store_nbr'].unique()\n",
    "unique_product_families = train['family'].unique()\n",
    "\n",
    "# Create a MultiIndex with all combinations of date, store_nbr, and family\n",
    "complete_index = pd.MultiIndex.from_tuples(\n",
    "    product(date_range, stores_list, unique_product_families),\n",
    "    names=['date', 'store_nbr', 'family']\n",
    ")\n",
    "\n",
    "# Create a complete dataframe\n",
    "complete_df = pd.DataFrame(index=complete_index).reset_index()\n",
    "\n",
    "# Merge with original train data\n",
    "train_complete = complete_df.merge(\n",
    "    train,\n",
    "    on=['date', 'store_nbr', 'family'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing values i.e. sales and onpromotion with 0\n",
    "train_complete['sales'] = train_complete['sales'].fillna(0)\n",
    "train_complete['onpromotion'] = train_complete['onpromotion'].fillna(0)\n",
    "\n",
    "# 4 missing dates, 54 stores, 33 product families i.e. 4 * 54 * 33 = 7128\n",
    "print(f\"Added records: {len(train_complete) - len(train)}\")\n",
    "\n",
    "train = train_complete"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added records: 7128\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "f6b9e5f4",
   "metadata": {},
   "source": [
    "### Check missing oil dates"
   ]
  },
  {
   "cell_type": "code",
   "id": "71bae50b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:51:57.306008Z",
     "start_time": "2025-11-26T09:51:57.293865Z"
    }
   },
   "source": [
    "# Get the date range from earliest to latest in train and test data\n",
    "date_range = pd.date_range(start=train['date'].min(), end=test['date'].max(), freq='D')\n",
    "\n",
    "# Get unique dates in oil data\n",
    "oil_dates = oil['date'].unique()\n",
    "\n",
    "# Find missing dates\n",
    "missing_dates = date_range.difference(pd.DatetimeIndex(oil_dates))\n",
    "\n",
    "print(f\"Total dates in range: {len(date_range)}\")\n",
    "print(f\"Unique dates in oil: {len(oil_dates)}\")\n",
    "print(f\"Missing dates: {len(missing_dates)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dates in range: 1704\n",
      "Unique dates in oil: 1218\n",
      "Missing dates: 486\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We are once again missing dates, which disrupts our continuous timeseries. We will use the same method to fix it.",
   "id": "375441b171bef850"
  },
  {
   "cell_type": "code",
   "id": "32564df9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:53:19.483738Z",
     "start_time": "2025-11-26T09:53:19.452498Z"
    }
   },
   "source": [
    "# Create complete date range for the data from train and test data\n",
    "date_range = pd.date_range(start=train['date'].min(), end=test['date'].max(), freq='D')\n",
    "\n",
    "# Create complete dataframe with all dates\n",
    "oil_complete = pd.DataFrame({'date': date_range})\n",
    "\n",
    "# Merge with original oil data\n",
    "oil_complete = oil_complete.merge(\n",
    "    oil, \n",
    "    on='date', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Forward fill (use last known price for missing dates)\n",
    "oil_complete['dcoilwtico'] = oil_complete['dcoilwtico'].ffill()\n",
    "\n",
    "# Backward fill for any remaining NaNs at the start\n",
    "oil_complete['dcoilwtico'] = oil_complete['dcoilwtico'].bfill()\n",
    "\n",
    "# Update oil dataframe\n",
    "print(f\"Number of records after filling missing oil data: {len(oil_complete)}\")\n",
    "print(f\"Added records: {len(oil_complete) - len(oil)}\")\n",
    "\n",
    "oil = oil_complete"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records after filling missing oil data: 1704\n",
      "Added records: 486\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "1dfcb5c5",
   "metadata": {},
   "source": [
    "### Check transactions"
   ]
  },
  {
   "cell_type": "code",
   "id": "52a01d57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:54:05.247200Z",
     "start_time": "2025-11-26T09:54:04.939127Z"
    }
   },
   "source": [
    "# Total transactions count\n",
    "total_transactions_count = len(train.groupby([\"date\", \"store_nbr\"])['sales'].sum())\n",
    "\n",
    "print(f\"Missing records from transactions: {total_transactions_count - len(transactions)}\")\n",
    "\n",
    "\n",
    "store_sales = train.groupby(['date', 'store_nbr'])['sales'].sum().reset_index()\n",
    "\n",
    "# Merge transactions with sales\n",
    "transactions_complete = transactions.merge(\n",
    "    store_sales,\n",
    "    on=['date', 'store_nbr'],\n",
    "    how='outer'\n",
    ").sort_values(by=['date', 'store_nbr'], ignore_index=True)\n",
    "\n",
    "# For dates where sales are 0, set transactions to 0\n",
    "transactions_complete.loc[transactions_complete[\"sales\"].eq(0), \"transactions\"] = 0\n",
    "\n",
    "# Interpolate missing transactions for each store\n",
    "transactions_complete['transactions'] = transactions_complete.groupby('store_nbr')['transactions'].transform(\n",
    "    lambda x: x.interpolate(method='linear')\n",
    ")\n",
    "\n",
    "# Round transactions to remove fractions from interpolation\n",
    "transactions_complete['transactions'] = transactions_complete['transactions'].round().astype(int)\n",
    "\n",
    "# Drop sales column\n",
    "transactions_complete = transactions_complete.drop('sales', axis=1)\n",
    "\n",
    "print(f\"Added transactions: {len(transactions_complete) - len(transactions)}\")\n",
    "\n",
    "transactions = transactions_complete"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing records from transactions: 7664\n",
      "Added transactions: 7664\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "7fa25219",
   "metadata": {},
   "source": [
    "### Holidays"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We want to create a 0/1 flag variable for holidays in our time series. This way we can account for extra store traffic during holidays quite simply.",
   "id": "1643d742c0c35da1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T17:01:17.438129Z",
     "start_time": "2025-11-27T17:01:15.170541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Clean holiday event descriptions and remove \"Transfer\" holidays, since they are actually not holidays.\n",
    "holidays_events[\"description\"] = holidays_events.apply(\n",
    "    lambda x: x[\"description\"].strip().lower().replace(x[\"locale_name\"].lower(), \"\"), axis=1\n",
    ").apply(\n",
    "    #If the holiday is connected to football, we will just categorize it as futbol\n",
    "    lambda x: \"futbol\" if \"futbol\" in x else x\n",
    ").replace(\n",
    "    #Removing unneeded words in the names of holidays\n",
    "    r'\\b(de|del|puente|traslado|recupero)\\b', '', regex=True\n",
    ").replace(\n",
    "    #Removing +1, -2 etc type holidays since these are not actual holidays\n",
    "    r'[+-]\\d+', '', regex=True\n",
    ").str.strip()\n",
    "#Transfers are not actually holidays, they have been moved to a different day.\n",
    "holidays_events = holidays_events[holidays_events[\"type\"] != \"Transfer\"]\n",
    "\n",
    "#Build 3 holiday tables for national, regional and local\n",
    "\n",
    "h = holidays_events[['date', 'locale', 'locale_name']].copy()\n",
    "\n",
    "# NATIONAL\n",
    "#We only need to keep the date, since nationals take place all over the country\n",
    "national = h[h['locale'] == 'National'][['date']].drop_duplicates()\n",
    "national['national_holiday'] = 1\n",
    "\n",
    "# REGIONAL → matches on state\n",
    "#we have to keep the date and the region\n",
    "regional = h[h['locale'] == 'Regional'][['date', 'locale_name']].drop_duplicates()\n",
    "regional = regional.rename(columns={'locale_name': 'state'})\n",
    "regional['regional_holiday'] = 1\n",
    "\n",
    "# LOCAL → matches on city\n",
    "#We have to keep the city and date\n",
    "local = h[h['locale'] == 'Local'][['date', 'locale_name']].drop_duplicates()\n",
    "local = local.rename(columns={'locale_name': 'city'})\n",
    "local['local_holiday'] = 1\n",
    "\n",
    "# --- Merge into data ---\n",
    "data = pd.concat(\n",
    "    [train, test], axis=0, ignore_index=True\n",
    ").merge(\n",
    "    transactions, on=['date', 'store_nbr'], how='left'\n",
    ").merge(\n",
    "    oil, on='date', how='left'\n",
    ").merge(\n",
    "    stores, on='store_nbr', how='left'\n",
    ")\n",
    "# Merge NATIONAL\n",
    "data = data.merge(national, on='date', how='left')\n",
    "\n",
    "# Merge REGIONAL\n",
    "data = data.merge(regional, on=['date', 'state'], how='left')\n",
    "\n",
    "# Merge LOCAL\n",
    "data = data.merge(local, on=['date', 'city'], how='left')\n",
    "\n",
    "# Final combined holiday flag column\n",
    "data['holiday'] = (\n",
    "    data[['national_holiday', 'regional_holiday', 'local_holiday']]\n",
    "    .fillna(0)\n",
    "    .max(axis=1)\n",
    "    .astype(int) #should maybe make into factor\n",
    ")\n",
    "\n",
    "# Cleanup\n",
    "data = data.drop(columns=['national_holiday', 'regional_holiday', 'local_holiday'])\n",
    "\n",
    "data['holiday'].value_counts()"
   ],
   "id": "fa8121b4bd10a904",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "holiday\n",
       "0    2774310\n",
       "1     262218\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "26c36448",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T16:50:20.362380Z",
     "start_time": "2025-11-27T16:50:20.246837Z"
    }
   },
   "source": [
    "holidays_events[\"description\"] = holidays_events.apply(\n",
    "    lambda x: x[\"description\"].strip().lower().replace(x[\"locale_name\"].lower(), \"\"), axis=1 # Remove names\n",
    ").apply(\n",
    "    lambda x: \"futbol\" if \"futbol\" in x else x # Only keep futbol\n",
    ").replace(r'\\b(de|del|puente|traslado|recupero)\\b', '', regex=True\n",
    ").replace(r'[+-]\\d+', '', regex=True # Remove digits with leading + or -\n",
    ").str.strip()\n",
    "\n",
    "holidays_events = holidays_events[holidays_events[\"type\"] != \"Transfer\"] # Transfer holidays are basically not actual holidays"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "80de5532",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T16:50:22.991788Z",
     "start_time": "2025-11-27T16:50:22.985983Z"
    }
   },
   "source": [
    "def check_holiday(row, holiday_events):\n",
    "    holiday_rows = holiday_events[holiday_events['date'] == row['date']]\n",
    "    \n",
    "    # There are no holidays\n",
    "    if holiday_rows.empty:\n",
    "        return 0\n",
    "    \n",
    "    # Check for National holiday\n",
    "    if not holiday_rows[holiday_rows['locale'] == 'National'].empty:\n",
    "        return 1\n",
    "    \n",
    "    # Check for Regional holiday (state must match)\n",
    "    regional_holidays = holiday_rows[holiday_rows['locale'] == 'Regional']\n",
    "    if not regional_holidays.empty:\n",
    "        if not regional_holidays[regional_holidays['locale_name'] == row['state']].empty:\n",
    "            return 1\n",
    "    \n",
    "    # Check for Local holiday (city must match)\n",
    "    local_holidays = holiday_rows[holiday_rows['locale'] == 'Local']\n",
    "    if not local_holidays.empty:\n",
    "        if not local_holidays[local_holidays['locale_name'] == row['city']].empty:\n",
    "            return 1\n",
    "\n",
    "    return 0"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "2a2e4393",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T16:50:51.361603Z",
     "start_time": "2025-11-27T16:50:26.077952Z"
    }
   },
   "source": [
    "data = pd.concat(\n",
    "    [train, test], axis=0, ignore_index=True\n",
    ").merge(\n",
    "    transactions, on=['date', 'store_nbr'], how='left'  \n",
    ").merge(\n",
    "    oil, on='date', how='left'\n",
    ").merge(\n",
    "    stores, on='store_nbr', how='left'\n",
    ")\n",
    "\n",
    "# This can be optimized by using a more efficient approach (e.g. merging)\n",
    "data[\"holiday\"] = data.apply(\n",
    "    lambda x: check_holiday(x, holidays_events), axis=1\n",
    ")"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 12\u001B[39m\n\u001B[32m      1\u001B[39m data = pd.concat(\n\u001B[32m      2\u001B[39m     [train, test], axis=\u001B[32m0\u001B[39m, ignore_index=\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m      3\u001B[39m ).merge(\n\u001B[32m   (...)\u001B[39m\u001B[32m      8\u001B[39m     stores, on=\u001B[33m'\u001B[39m\u001B[33mstore_nbr\u001B[39m\u001B[33m'\u001B[39m, how=\u001B[33m'\u001B[39m\u001B[33mleft\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m      9\u001B[39m )\n\u001B[32m     11\u001B[39m \u001B[38;5;66;03m# This can be optimized by using a more efficient approach (e.g. merging)\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m data[\u001B[33m\"\u001B[39m\u001B[33mholiday\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_holiday\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mholidays_events\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\n\u001B[32m     14\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/pandas/core/frame.py:10381\u001B[39m, in \u001B[36mDataFrame.apply\u001B[39m\u001B[34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001B[39m\n\u001B[32m  10367\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcore\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mapply\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m frame_apply\n\u001B[32m  10369\u001B[39m op = frame_apply(\n\u001B[32m  10370\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m  10371\u001B[39m     func=func,\n\u001B[32m   (...)\u001B[39m\u001B[32m  10379\u001B[39m     kwargs=kwargs,\n\u001B[32m  10380\u001B[39m )\n\u001B[32m> \u001B[39m\u001B[32m10381\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mop\u001B[49m\u001B[43m.\u001B[49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m.__finalize__(\u001B[38;5;28mself\u001B[39m, method=\u001B[33m\"\u001B[39m\u001B[33mapply\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/pandas/core/apply.py:916\u001B[39m, in \u001B[36mFrameApply.apply\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    913\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.raw:\n\u001B[32m    914\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.apply_raw(engine=\u001B[38;5;28mself\u001B[39m.engine, engine_kwargs=\u001B[38;5;28mself\u001B[39m.engine_kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m916\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/pandas/core/apply.py:1063\u001B[39m, in \u001B[36mFrameApply.apply_standard\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1061\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mapply_standard\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m   1062\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.engine == \u001B[33m\"\u001B[39m\u001B[33mpython\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m1063\u001B[39m         results, res_index = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mapply_series_generator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1064\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1065\u001B[39m         results, res_index = \u001B[38;5;28mself\u001B[39m.apply_series_numba()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/pandas/core/apply.py:1081\u001B[39m, in \u001B[36mFrameApply.apply_series_generator\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1078\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m option_context(\u001B[33m\"\u001B[39m\u001B[33mmode.chained_assignment\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m   1079\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m i, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(series_gen):\n\u001B[32m   1080\u001B[39m         \u001B[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1081\u001B[39m         results[i] = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1082\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(results[i], ABCSeries):\n\u001B[32m   1083\u001B[39m             \u001B[38;5;66;03m# If we have a view on v, we need to make a copy because\u001B[39;00m\n\u001B[32m   1084\u001B[39m             \u001B[38;5;66;03m#  series_generator will swap out the underlying data\u001B[39;00m\n\u001B[32m   1085\u001B[39m             results[i] = results[i].copy(deep=\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 13\u001B[39m, in \u001B[36m<lambda>\u001B[39m\u001B[34m(x)\u001B[39m\n\u001B[32m      1\u001B[39m data = pd.concat(\n\u001B[32m      2\u001B[39m     [train, test], axis=\u001B[32m0\u001B[39m, ignore_index=\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m      3\u001B[39m ).merge(\n\u001B[32m   (...)\u001B[39m\u001B[32m      8\u001B[39m     stores, on=\u001B[33m'\u001B[39m\u001B[33mstore_nbr\u001B[39m\u001B[33m'\u001B[39m, how=\u001B[33m'\u001B[39m\u001B[33mleft\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m      9\u001B[39m )\n\u001B[32m     11\u001B[39m \u001B[38;5;66;03m# This can be optimized by using a more efficient approach (e.g. merging)\u001B[39;00m\n\u001B[32m     12\u001B[39m data[\u001B[33m\"\u001B[39m\u001B[33mholiday\u001B[39m\u001B[33m\"\u001B[39m] = data.apply(\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m     \u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[43mcheck_holiday\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mholidays_events\u001B[49m\u001B[43m)\u001B[49m, axis=\u001B[32m1\u001B[39m\n\u001B[32m     14\u001B[39m )\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[17]\u001B[39m\u001B[32m, line 2\u001B[39m, in \u001B[36mcheck_holiday\u001B[39m\u001B[34m(row, holiday_events)\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcheck_holiday\u001B[39m(row, holiday_events):\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m     holiday_rows = \u001B[43mholiday_events\u001B[49m\u001B[43m[\u001B[49m\u001B[43mholiday_events\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mdate\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[43m==\u001B[49m\u001B[43m \u001B[49m\u001B[43mrow\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mdate\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n\u001B[32m      4\u001B[39m     \u001B[38;5;66;03m# There are no holidays\u001B[39;00m\n\u001B[32m      5\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m holiday_rows.empty:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/pandas/core/frame.py:4097\u001B[39m, in \u001B[36mDataFrame.__getitem__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   4094\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.where(key)\n\u001B[32m   4096\u001B[39m \u001B[38;5;66;03m# Do we have a (boolean) 1d indexer?\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m4097\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mcom\u001B[49m\u001B[43m.\u001B[49m\u001B[43mis_bool_indexer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m   4098\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._getitem_bool_array(key)\n\u001B[32m   4100\u001B[39m \u001B[38;5;66;03m# We are left with two options: a single key, and a collection of keys,\u001B[39;00m\n\u001B[32m   4101\u001B[39m \u001B[38;5;66;03m# We interpret tuples as collections only for non-MultiIndex\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/pandas/core/common.py:139\u001B[39m, in \u001B[36mis_bool_indexer\u001B[39m\u001B[34m(key)\u001B[39m\n\u001B[32m    137\u001B[39m             \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m    138\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m139\u001B[39m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[43mis_bool_dtype\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m    140\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    141\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, \u001B[38;5;28mlist\u001B[39m):\n\u001B[32m    142\u001B[39m     \u001B[38;5;66;03m# check if np.array(key).dtype would be bool\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/pandas/core/dtypes/common.py:1252\u001B[39m, in \u001B[36mis_bool_dtype\u001B[39m\u001B[34m(arr_or_dtype)\u001B[39m\n\u001B[32m   1249\u001B[39m     arr_or_dtype = dtype.categories\n\u001B[32m   1250\u001B[39m     \u001B[38;5;66;03m# now we use the special definition for Index\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1252\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43marr_or_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mABCIndex\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m   1253\u001B[39m     \u001B[38;5;66;03m# Allow Index[object] that is all-bools or Index[\"boolean\"]\u001B[39;00m\n\u001B[32m   1254\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m arr_or_dtype.inferred_type == \u001B[33m\"\u001B[39m\u001B[33mboolean\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m   1255\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_bool_dtype(arr_or_dtype.dtype):\n\u001B[32m   1256\u001B[39m             \u001B[38;5;66;03m# GH#52680\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/lib/python3.12/site-packages/pandas/core/dtypes/generic.py:42\u001B[39m, in \u001B[36mcreate_pandas_abc_type.<locals>._instancecheck\u001B[39m\u001B[34m(cls, inst)\u001B[39m\n\u001B[32m     38\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(inst, attr, \u001B[33m\"\u001B[39m\u001B[33m_typ\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01min\u001B[39;00m comp\n\u001B[32m     40\u001B[39m \u001B[38;5;66;03m# https://github.com/python/mypy/issues/1006\u001B[39;00m\n\u001B[32m     41\u001B[39m \u001B[38;5;66;03m# error: 'classmethod' used with a non-method\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m42\u001B[39m \u001B[38;5;129m@classmethod\u001B[39m  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m     43\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_instancecheck\u001B[39m(\u001B[38;5;28mcls\u001B[39m, inst) -> \u001B[38;5;28mbool\u001B[39m:\n\u001B[32m     44\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _check(inst) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(inst, \u001B[38;5;28mtype\u001B[39m)\n\u001B[32m     46\u001B[39m \u001B[38;5;129m@classmethod\u001B[39m  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m     47\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_subclasscheck\u001B[39m(\u001B[38;5;28mcls\u001B[39m, inst) -> \u001B[38;5;28mbool\u001B[39m:\n\u001B[32m     48\u001B[39m     \u001B[38;5;66;03m# Raise instead of returning False\u001B[39;00m\n\u001B[32m     49\u001B[39m     \u001B[38;5;66;03m# This is consistent with default __subclasscheck__ behavior\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "2e5c9a14",
   "metadata": {},
   "source": [
    "### Creating one and two day lags for dcoilwtico"
   ]
  },
  {
   "cell_type": "code",
   "id": "f42a9765",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T17:01:32.759357Z",
     "start_time": "2025-11-27T17:01:31.328167Z"
    }
   },
   "source": [
    "# Sort by store_nbr, family, and date to ensure correct order within each group\n",
    "data = data.sort_values(['store_nbr', 'family', 'date'])\n",
    "\n",
    "# Create lag-1 column within each store-family combination\n",
    "data['lag-1-dcoilwtico'] = data.groupby(['store_nbr', 'family'])['dcoilwtico'].shift(1)\n",
    "\n",
    "# Create lag-2 column (2 days ago)\n",
    "data['lag-2-dcoilwtico'] = data.groupby(['store_nbr', 'family'])['dcoilwtico'].shift(2)\n",
    "\n",
    "# Backfill NaNs at the start of each group\n",
    "data['lag-1-dcoilwtico'] = data.groupby(['store_nbr', 'family'])['lag-1-dcoilwtico'].bfill()\n",
    "data['lag-2-dcoilwtico'] = data.groupby(['store_nbr', 'family'])['lag-2-dcoilwtico'].bfill()\n",
    "data = data.sort_values('date')"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "eab82a7a",
   "metadata": {},
   "source": [
    "### Extract date features"
   ]
  },
  {
   "cell_type": "code",
   "id": "32ab3675",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T17:01:36.004760Z",
     "start_time": "2025-11-27T17:01:35.558332Z"
    }
   },
   "source": [
    "data['year'] = data['date'].dt.year\n",
    "data['month'] = data['date'].dt.month\n",
    "data['day'] = data['date'].dt.day\n",
    "data['day_of_week'] = data['date'].dt.dayofweek\n",
    "data['day_of_year'] = data['date'].dt.dayofyear\n",
    "data['week_of_year'] = data['date'].dt.isocalendar().week\n",
    "data['is_weekend'] = (data['date'].dt.dayofweek >= 5).astype(int)"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "96f579f3",
   "metadata": {},
   "source": [
    "### Label Encoding for categorical features"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Since most ML models cannot work with categorical features like \"AUTOMOTIVE\" etc, we have to numerically encode them for the model to work.",
   "id": "81fbb111f774a079"
  },
  {
   "cell_type": "code",
   "id": "7e9bbb6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T17:01:43.943246Z",
     "start_time": "2025-11-27T17:01:42.758983Z"
    }
   },
   "source": [
    "# Get all unique values for each categorical column from data\n",
    "all_product_families = data['family'].unique()\n",
    "all_cities = data['city'].unique()\n",
    "all_states = data['state'].unique()\n",
    "all_types = data['type'].unique()\n",
    "\n",
    "# Initialize label encoders\n",
    "le_product_family = LabelEncoder()\n",
    "le_city = LabelEncoder()\n",
    "le_state = LabelEncoder()\n",
    "le_type = LabelEncoder()\n",
    "\n",
    "# Fit on all unique values\n",
    "le_product_family.fit(all_product_families)\n",
    "le_city.fit(all_cities)\n",
    "le_state.fit(all_states)\n",
    "le_type.fit(all_types)\n",
    "\n",
    "# Transform data dataframe\n",
    "data['family'] = le_product_family.transform(data['family'])\n",
    "data['city'] = le_city.transform(data['city'])\n",
    "data['state'] = le_state.transform(data['state'])\n",
    "data['type'] = le_type.transform(data['type'])\n",
    "\n",
    "print(\"Label encoding complete!\")\n",
    "print(f\"Family: {len(all_product_families)} categories\")\n",
    "print(f\"City: {len(all_cities)} categories\")\n",
    "print(f\"State: {len(all_states)} categories\")\n",
    "print(f\"Type: {len(all_types)} categories\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoding complete!\n",
      "Family: 33 categories\n",
      "City: 22 categories\n",
      "State: 16 categories\n",
      "Type: 5 categories\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "56ae8e71",
   "metadata": {},
   "source": [
    "### Save final version of 'data' dataframe "
   ]
  },
  {
   "cell_type": "code",
   "id": "8e90d795",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T17:02:06.320477Z",
     "start_time": "2025-11-27T17:01:50.158026Z"
    }
   },
   "source": [
    "data.to_csv('./data/data.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "5fb22739",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67725cad",
   "metadata": {},
   "source": [
    "### Splitting test and train data"
   ]
  },
  {
   "cell_type": "code",
   "id": "e6ca108d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T17:03:40.312544Z",
     "start_time": "2025-11-27T17:03:38.951916Z"
    }
   },
   "source": [
    "# Split based on date\n",
    "train = data[data['date'] < '2017-08-16'].copy()\n",
    "test = data[data['date'] >= '2017-08-16'].copy()\n",
    "\n",
    "# Save test IDs before dropping\n",
    "test_ids = test['id'].copy()\n",
    "\n",
    "# Drop transactions column from both\n",
    "train = train.drop('transactions', axis=1)\n",
    "test = test.drop('transactions', axis=1)\n",
    "\n",
    "# Drop date column since we have date features\n",
    "train = train.drop('date', axis=1)\n",
    "test = test.drop('date', axis=1)\n",
    "\n",
    "# Drop id column\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)\n",
    "\n",
    "# Drop sales column only from test\n",
    "test = test.drop('sales', axis=1)\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"Test IDs saved: {len(test_ids)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (3008016, 19)\n",
      "Test shape: (28512, 18)\n",
      "Test IDs saved: 28512\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "d12a9368",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T17:03:56.664472Z",
     "start_time": "2025-11-27T17:03:56.424432Z"
    }
   },
   "source": [
    "# Separate features and target\n",
    "X_train = train.drop('sales', axis=1)\n",
    "y_train = train['sales']\n",
    "\n",
    "X_test = test \n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Target: {y_train.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (3008016, 18)\n",
      "Test set: (28512, 18)\n",
      "Target: (3008016,)\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "5fc8877c",
   "metadata": {},
   "source": [
    "### Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "id": "2ced950f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T17:04:08.183519Z",
     "start_time": "2025-11-27T17:04:08.178524Z"
    }
   },
   "source": [
    "def train_random_forest_model(X_train, y_train):\n",
    "    # Initialize Random Forest model\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=20,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=4,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training Random Forest model...\")\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on training set to evaluate\n",
    "    y_train_pred_rf = np.maximum(0, rf_model.predict(X_train))\n",
    "\n",
    "    # Calculate metrics\n",
    "    rmsle_score = np.sqrt(mean_squared_log_error(y_train, y_train_pred_rf))\n",
    "\n",
    "    return rf_model, rmsle_score"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "781d3f5f",
   "metadata": {},
   "source": [
    "### LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "b9bdd511",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T17:04:14.708127Z",
     "start_time": "2025-11-27T17:04:14.704839Z"
    }
   },
   "source": [
    "def train_lightgbm_model(X_train, y_train):\n",
    "    # Initialize LightGBM model\n",
    "    lgb_model = LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=20,\n",
    "        learning_rate=0.1,\n",
    "        num_leaves=31,\n",
    "        min_child_samples=20,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training LightGBM model...\")\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on training set to evaluate\n",
    "    y_train_pred_lgb = np.maximum(0, lgb_model.predict(X_train))\n",
    "\n",
    "    # Calculate metrics\n",
    "    rmsle_score_lgb = np.sqrt(mean_squared_log_error(y_train, y_train_pred_lgb))\n",
    "\n",
    "    return lgb_model, rmsle_score_lgb"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "id": "05df1d02",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "a4270622",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T17:04:17.881126Z",
     "start_time": "2025-11-27T17:04:17.877751Z"
    }
   },
   "source": [
    "def train_xgboost_model(X_train, y_train):\n",
    "    # Initialize XGBoost model\n",
    "    xgb_model = XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        learning_rate=0.1,\n",
    "        min_child_weight=3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbosity=1\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training XGBoost model...\")\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on training set to evaluate\n",
    "    y_train_pred_xgb = np.maximum(0, xgb_model.predict(X_train))\n",
    "\n",
    "    # Calculate metrics\n",
    "    rmsle_score_xgb = np.sqrt(mean_squared_log_error(y_train, y_train_pred_xgb))\n",
    "\n",
    "    return xgb_model, rmsle_score_xgb"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "2d3c7bf7",
   "metadata": {},
   "source": [
    "### Create submission"
   ]
  },
  {
   "cell_type": "code",
   "id": "5dbfa36e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T17:04:25.189265Z",
     "start_time": "2025-11-27T17:04:25.186172Z"
    }
   },
   "source": [
    "def create_submission(predictions, test_ids, filename='submission.csv'):\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_ids.values.astype(int),\n",
    "        'sales': predictions\n",
    "    })\n",
    "    \n",
    "    submission.to_csv(filename, index=False)\n",
    "    return submission"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "id": "7fbbd718",
   "metadata": {},
   "source": [
    "### Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "id": "9d5b159c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T17:12:21.229702Z",
     "start_time": "2025-11-27T17:04:30.462484Z"
    }
   },
   "source": [
    "xgb_model, xgb_rmsle_score = train_xgboost_model(X_train, y_train)\n",
    "rf_model, rf_rmsle_score = train_random_forest_model(X_train, y_train)\n",
    "lgbm_model, lgbm_rmsle_score = train_lightgbm_model(X_train, y_train)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost model...\n",
      "Training Random Forest model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  7.3min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:   11.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM model...\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "e54eba17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T17:15:26.486387Z",
     "start_time": "2025-11-27T17:15:26.455163Z"
    }
   },
   "source": [
    "# Compare all models including ensemble\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'LightGBM', 'XGBoost'],\n",
    "    'RMSLE': [rf_rmsle_score, lgbm_rmsle_score, xgb_rmsle_score]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(f\"\\nBest model by RMSLE: {comparison_df.loc[comparison_df['RMSLE'].idxmin(), 'Model']}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance Comparison:\n",
      "        Model    RMSLE\n",
      "Random Forest 0.625781\n",
      "     LightGBM 2.154993\n",
      "      XGBoost 1.505900\n",
      "\n",
      "Best model by RMSLE: Random Forest\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "id": "f1e3cea4",
   "metadata": {},
   "source": [
    "### Create submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7987718b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All submission files created successfully!\n"
     ]
    }
   ],
   "source": [
    "xgb_y_pred = xgb_model.predict(X_test)\n",
    "rf_y_pred = rf_model.predict(X_test)\n",
    "lgbm_y_pred = lgbm_model.predict(X_test)\n",
    "\n",
    "# Create submission files using the saved test IDs\n",
    "xgb_submission = create_submission(xgb_y_pred, test_ids, 'xgb_submission.csv')\n",
    "rf_submission = create_submission(rf_y_pred, test_ids, 'rf_submission.csv')\n",
    "lgbm_submission = create_submission(lgbm_y_pred, test_ids, 'lgbm_submission.csv')\n",
    "\n",
    "print(\"All submission files created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
